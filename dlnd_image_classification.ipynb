{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9a012f208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_normed = (x - x.min(0)) / x.ptp(0)\n",
    "    return(x_normed)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    rows = len(x)      \n",
    "    result = np.zeros([rows,10])\n",
    "    for i, j in  enumerate(x):  \n",
    "       result[i][j] = 1\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input = tf.placeholder(tf.float32,\n",
    "    shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "    return(input)\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "    return(input)\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input = tf.placeholder(tf.float32, shape=None, name='keep_prob')\n",
    "    return(input)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    third=x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal(\n",
    "    [conv_ksize[0], conv_ksize[1], third, conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, \n",
    "                              strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # Max pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "    strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "    padding='SAME')\n",
    "\n",
    "    return(conv_layer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dim = x_tensor.get_shape().as_list()\n",
    "    output = tf.reshape(x_tensor, \n",
    "                       [-1, dim[1] * dim[2] * dim[3]])\n",
    "    return(output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dims = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([dims[1], num_outputs], mean=0.0, stddev=0.01))\n",
    "    biases = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    return(fc1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dims = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([dims[1], num_outputs], mean=0.0, stddev=0.01))\n",
    "    biases = tf.Variable(tf.zeros([num_outputs]))\n",
    "\n",
    "    out = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    return(out)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor = x\n",
    "    conv_ksize = (3,3) \n",
    "    conv_num_outputs = 10\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (3,3)\n",
    "    pool_strides = (1,1)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    conv = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2 = conv2d_maxpool(conv, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv2)\n",
    "    flat = tf.nn.dropout(flat, keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(flat, num_outputs)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, num_outputs)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    result = output(fc2, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability})\n",
    "\n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    \n",
    "    acc = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.})\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2382 Validation Accuracy: 0.150000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2040 Validation Accuracy: 0.150000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1832 Validation Accuracy: 0.150000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1556 Validation Accuracy: 0.200000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1249 Validation Accuracy: 0.175000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1179 Validation Accuracy: 0.175000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0782 Validation Accuracy: 0.175000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0714 Validation Accuracy: 0.225000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0850 Validation Accuracy: 0.175000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0574 Validation Accuracy: 0.200000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.0274 Validation Accuracy: 0.225000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.0333 Validation Accuracy: 0.125000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.0171 Validation Accuracy: 0.225000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     2.0102 Validation Accuracy: 0.200000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.0093 Validation Accuracy: 0.200000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.0056 Validation Accuracy: 0.225000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.9888 Validation Accuracy: 0.225000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.9747 Validation Accuracy: 0.225000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.9777 Validation Accuracy: 0.200000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.9505 Validation Accuracy: 0.250000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.9380 Validation Accuracy: 0.275000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.9324 Validation Accuracy: 0.250000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.9264 Validation Accuracy: 0.250000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.9097 Validation Accuracy: 0.250000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.9239 Validation Accuracy: 0.275000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.9147 Validation Accuracy: 0.200000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.8791 Validation Accuracy: 0.225000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.8961 Validation Accuracy: 0.200000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.8570 Validation Accuracy: 0.225000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.8664 Validation Accuracy: 0.225000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.8668 Validation Accuracy: 0.200000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.8524 Validation Accuracy: 0.200000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.9526 Validation Accuracy: 0.175000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.8848 Validation Accuracy: 0.175000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.8709 Validation Accuracy: 0.200000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.8960 Validation Accuracy: 0.175000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.8548 Validation Accuracy: 0.175000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.8469 Validation Accuracy: 0.175000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.8612 Validation Accuracy: 0.200000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.8284 Validation Accuracy: 0.200000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.8328 Validation Accuracy: 0.200000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.8099 Validation Accuracy: 0.225000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.8291 Validation Accuracy: 0.225000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.8323 Validation Accuracy: 0.225000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.8053 Validation Accuracy: 0.200000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.7840 Validation Accuracy: 0.225000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.7610 Validation Accuracy: 0.225000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.7707 Validation Accuracy: 0.200000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.7344 Validation Accuracy: 0.225000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.7649 Validation Accuracy: 0.275000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.7615 Validation Accuracy: 0.250000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.7907 Validation Accuracy: 0.225000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.7385 Validation Accuracy: 0.225000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.7440 Validation Accuracy: 0.325000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.7249 Validation Accuracy: 0.300000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.7162 Validation Accuracy: 0.250000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.7290 Validation Accuracy: 0.200000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.7272 Validation Accuracy: 0.175000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.7137 Validation Accuracy: 0.200000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.7203 Validation Accuracy: 0.175000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.7193 Validation Accuracy: 0.175000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.7131 Validation Accuracy: 0.225000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.7122 Validation Accuracy: 0.200000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.7244 Validation Accuracy: 0.200000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.7563 Validation Accuracy: 0.175000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.7074 Validation Accuracy: 0.250000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.6935 Validation Accuracy: 0.225000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.6870 Validation Accuracy: 0.225000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.7015 Validation Accuracy: 0.175000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.6912 Validation Accuracy: 0.150000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.7142 Validation Accuracy: 0.225000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.7105 Validation Accuracy: 0.200000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.7320 Validation Accuracy: 0.225000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.6819 Validation Accuracy: 0.250000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.6819 Validation Accuracy: 0.225000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.6706 Validation Accuracy: 0.175000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.6937 Validation Accuracy: 0.225000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.6814 Validation Accuracy: 0.250000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.6467 Validation Accuracy: 0.225000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.6291 Validation Accuracy: 0.250000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.6149 Validation Accuracy: 0.275000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.6343 Validation Accuracy: 0.250000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.6337 Validation Accuracy: 0.250000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.5910 Validation Accuracy: 0.250000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.5839 Validation Accuracy: 0.225000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.5933 Validation Accuracy: 0.275000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.6030 Validation Accuracy: 0.275000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.5656 Validation Accuracy: 0.250000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.5886 Validation Accuracy: 0.275000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.5783 Validation Accuracy: 0.225000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.5665 Validation Accuracy: 0.250000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.5438 Validation Accuracy: 0.250000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.6389 Validation Accuracy: 0.250000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.5581 Validation Accuracy: 0.225000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.5718 Validation Accuracy: 0.325000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.5484 Validation Accuracy: 0.225000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.6048 Validation Accuracy: 0.325000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.6067 Validation Accuracy: 0.275000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.5647 Validation Accuracy: 0.350000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.5339 Validation Accuracy: 0.250000\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.5431 Validation Accuracy: 0.300000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.5278 Validation Accuracy: 0.275000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.5314 Validation Accuracy: 0.275000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.5627 Validation Accuracy: 0.325000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.5453 Validation Accuracy: 0.350000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.5703 Validation Accuracy: 0.275000\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.5327 Validation Accuracy: 0.300000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.5444 Validation Accuracy: 0.275000\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.5631 Validation Accuracy: 0.300000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.5295 Validation Accuracy: 0.275000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.5440 Validation Accuracy: 0.300000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.5518 Validation Accuracy: 0.350000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.5192 Validation Accuracy: 0.350000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.5301 Validation Accuracy: 0.325000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.4999 Validation Accuracy: 0.300000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.5304 Validation Accuracy: 0.300000\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.5391 Validation Accuracy: 0.300000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.5093 Validation Accuracy: 0.325000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.5131 Validation Accuracy: 0.350000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.5369 Validation Accuracy: 0.350000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.5191 Validation Accuracy: 0.325000\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.4837 Validation Accuracy: 0.325000\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.5248 Validation Accuracy: 0.325000\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.5101 Validation Accuracy: 0.325000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.5112 Validation Accuracy: 0.325000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.4982 Validation Accuracy: 0.325000\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.5414 Validation Accuracy: 0.325000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.5271 Validation Accuracy: 0.375000\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.5350 Validation Accuracy: 0.375000\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.5122 Validation Accuracy: 0.350000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.4993 Validation Accuracy: 0.350000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.5245 Validation Accuracy: 0.275000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.5127 Validation Accuracy: 0.300000\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.5040 Validation Accuracy: 0.325000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.5462 Validation Accuracy: 0.300000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.5059 Validation Accuracy: 0.350000\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.5193 Validation Accuracy: 0.325000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.5197 Validation Accuracy: 0.350000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.4995 Validation Accuracy: 0.350000\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.5006 Validation Accuracy: 0.325000\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.5337 Validation Accuracy: 0.325000\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.5155 Validation Accuracy: 0.325000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.4995 Validation Accuracy: 0.325000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.5022 Validation Accuracy: 0.375000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.5204 Validation Accuracy: 0.325000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.5272 Validation Accuracy: 0.300000\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.5040 Validation Accuracy: 0.300000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.5012 Validation Accuracy: 0.350000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.4739 Validation Accuracy: 0.325000\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.4848 Validation Accuracy: 0.350000\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.4814 Validation Accuracy: 0.325000\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.4929 Validation Accuracy: 0.350000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.4834 Validation Accuracy: 0.325000\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.4952 Validation Accuracy: 0.375000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.5013 Validation Accuracy: 0.325000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.4762 Validation Accuracy: 0.300000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.4804 Validation Accuracy: 0.350000\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.4917 Validation Accuracy: 0.300000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.5271 Validation Accuracy: 0.350000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.4911 Validation Accuracy: 0.325000\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.4632 Validation Accuracy: 0.350000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.4812 Validation Accuracy: 0.350000\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.4872 Validation Accuracy: 0.450000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.4473 Validation Accuracy: 0.375000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.4756 Validation Accuracy: 0.325000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.4696 Validation Accuracy: 0.400000\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     1.4758 Validation Accuracy: 0.375000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     1.4626 Validation Accuracy: 0.400000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.4790 Validation Accuracy: 0.400000\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.4589 Validation Accuracy: 0.375000\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     1.4680 Validation Accuracy: 0.350000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     1.4627 Validation Accuracy: 0.400000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     1.4657 Validation Accuracy: 0.325000\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.5281 Validation Accuracy: 0.375000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     1.4518 Validation Accuracy: 0.350000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.4430 Validation Accuracy: 0.400000\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     1.4549 Validation Accuracy: 0.450000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     1.4855 Validation Accuracy: 0.350000\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     1.4467 Validation Accuracy: 0.350000\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     1.4464 Validation Accuracy: 0.375000\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     1.4599 Validation Accuracy: 0.400000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     1.4474 Validation Accuracy: 0.350000\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     1.4503 Validation Accuracy: 0.300000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     1.4805 Validation Accuracy: 0.350000\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     1.4615 Validation Accuracy: 0.350000\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     1.4474 Validation Accuracy: 0.375000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     1.4794 Validation Accuracy: 0.325000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     1.4568 Validation Accuracy: 0.350000\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     1.4760 Validation Accuracy: 0.350000\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     1.5031 Validation Accuracy: 0.350000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     1.4588 Validation Accuracy: 0.325000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     1.4456 Validation Accuracy: 0.350000\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     1.4650 Validation Accuracy: 0.300000\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     1.4797 Validation Accuracy: 0.375000\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     1.4709 Validation Accuracy: 0.300000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     1.5111 Validation Accuracy: 0.350000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     1.4836 Validation Accuracy: 0.325000\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     1.4522 Validation Accuracy: 0.400000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     1.4709 Validation Accuracy: 0.350000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     1.4469 Validation Accuracy: 0.325000\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     1.4736 Validation Accuracy: 0.300000\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:     1.4372 Validation Accuracy: 0.400000\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:     1.4513 Validation Accuracy: 0.350000\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:     1.4598 Validation Accuracy: 0.375000\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:     1.4270 Validation Accuracy: 0.375000\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:     1.4220 Validation Accuracy: 0.400000\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:     1.4494 Validation Accuracy: 0.350000\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:     1.4458 Validation Accuracy: 0.350000\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:     1.4365 Validation Accuracy: 0.375000\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:     1.4548 Validation Accuracy: 0.350000\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:     1.4160 Validation Accuracy: 0.400000\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:     1.4506 Validation Accuracy: 0.450000\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:     1.4153 Validation Accuracy: 0.400000\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:     1.4470 Validation Accuracy: 0.425000\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:     1.4395 Validation Accuracy: 0.400000\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:     1.4481 Validation Accuracy: 0.350000\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:     1.4440 Validation Accuracy: 0.350000\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:     1.4488 Validation Accuracy: 0.400000\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:     1.4130 Validation Accuracy: 0.350000\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:     1.4491 Validation Accuracy: 0.375000\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:     1.3982 Validation Accuracy: 0.375000\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:     1.4228 Validation Accuracy: 0.325000\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:     1.3915 Validation Accuracy: 0.375000\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:     1.4342 Validation Accuracy: 0.350000\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:     1.4401 Validation Accuracy: 0.375000\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:     1.4397 Validation Accuracy: 0.325000\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:     1.4038 Validation Accuracy: 0.350000\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:     1.4191 Validation Accuracy: 0.375000\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:     1.4099 Validation Accuracy: 0.325000\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:     1.4333 Validation Accuracy: 0.325000\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:     1.4110 Validation Accuracy: 0.350000\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:     1.4574 Validation Accuracy: 0.325000\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:     1.4287 Validation Accuracy: 0.350000\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:     1.4181 Validation Accuracy: 0.375000\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:     1.3911 Validation Accuracy: 0.300000\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:     1.4291 Validation Accuracy: 0.350000\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:     1.3968 Validation Accuracy: 0.375000\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:     1.3849 Validation Accuracy: 0.400000\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:     1.4509 Validation Accuracy: 0.350000\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:     1.4192 Validation Accuracy: 0.400000\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:     1.4124 Validation Accuracy: 0.350000\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:     1.4027 Validation Accuracy: 0.375000\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:     1.4156 Validation Accuracy: 0.350000\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:     1.4101 Validation Accuracy: 0.375000\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:     1.4241 Validation Accuracy: 0.400000\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:     1.4193 Validation Accuracy: 0.350000\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:     1.3912 Validation Accuracy: 0.375000\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:     1.4406 Validation Accuracy: 0.350000\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:     1.3921 Validation Accuracy: 0.375000\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:     1.3959 Validation Accuracy: 0.350000\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:     1.4359 Validation Accuracy: 0.350000\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:     1.3998 Validation Accuracy: 0.350000\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:     1.4158 Validation Accuracy: 0.400000\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:     1.4387 Validation Accuracy: 0.325000\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:     1.4315 Validation Accuracy: 0.350000\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:     1.4393 Validation Accuracy: 0.325000\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:     1.4025 Validation Accuracy: 0.325000\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:     1.4083 Validation Accuracy: 0.350000\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:     1.3881 Validation Accuracy: 0.350000\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:     1.3917 Validation Accuracy: 0.375000\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:     1.4002 Validation Accuracy: 0.300000\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:     1.4234 Validation Accuracy: 0.375000\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:     1.3918 Validation Accuracy: 0.325000\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:     1.3875 Validation Accuracy: 0.400000\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:     1.4247 Validation Accuracy: 0.375000\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:     1.4191 Validation Accuracy: 0.375000\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:     1.4465 Validation Accuracy: 0.350000\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:     1.4023 Validation Accuracy: 0.350000\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:     1.3887 Validation Accuracy: 0.375000\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:     1.3924 Validation Accuracy: 0.425000\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:     1.4080 Validation Accuracy: 0.400000\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:     1.4157 Validation Accuracy: 0.375000\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:     1.3898 Validation Accuracy: 0.400000\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:     1.4156 Validation Accuracy: 0.425000\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:     1.4058 Validation Accuracy: 0.375000\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:     1.4464 Validation Accuracy: 0.350000\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:     1.4377 Validation Accuracy: 0.400000\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:     1.4142 Validation Accuracy: 0.375000\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:     1.4360 Validation Accuracy: 0.350000\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:     1.4212 Validation Accuracy: 0.375000\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:     1.4271 Validation Accuracy: 0.350000\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:     1.4330 Validation Accuracy: 0.350000\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:     1.4132 Validation Accuracy: 0.375000\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:     1.4055 Validation Accuracy: 0.375000\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:     1.4218 Validation Accuracy: 0.350000\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:     1.4305 Validation Accuracy: 0.375000\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:     1.4448 Validation Accuracy: 0.375000\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:     1.4079 Validation Accuracy: 0.425000\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:     1.4043 Validation Accuracy: 0.425000\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:     1.4003 Validation Accuracy: 0.425000\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:     1.4308 Validation Accuracy: 0.400000\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:     1.4536 Validation Accuracy: 0.375000\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:     1.3894 Validation Accuracy: 0.400000\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:     1.4097 Validation Accuracy: 0.350000\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:     1.4422 Validation Accuracy: 0.300000\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:     1.4311 Validation Accuracy: 0.400000\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:     1.4207 Validation Accuracy: 0.350000\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:     1.4193 Validation Accuracy: 0.375000\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:     1.4146 Validation Accuracy: 0.350000\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:     1.4289 Validation Accuracy: 0.325000\n",
      "Epoch 301, CIFAR-10 Batch 1:  Loss:     1.4236 Validation Accuracy: 0.350000\n",
      "Epoch 302, CIFAR-10 Batch 1:  Loss:     1.4146 Validation Accuracy: 0.375000\n",
      "Epoch 303, CIFAR-10 Batch 1:  Loss:     1.3883 Validation Accuracy: 0.400000\n",
      "Epoch 304, CIFAR-10 Batch 1:  Loss:     1.4066 Validation Accuracy: 0.350000\n",
      "Epoch 305, CIFAR-10 Batch 1:  Loss:     1.3981 Validation Accuracy: 0.350000\n",
      "Epoch 306, CIFAR-10 Batch 1:  Loss:     1.4044 Validation Accuracy: 0.375000\n",
      "Epoch 307, CIFAR-10 Batch 1:  Loss:     1.4283 Validation Accuracy: 0.375000\n",
      "Epoch 308, CIFAR-10 Batch 1:  Loss:     1.4448 Validation Accuracy: 0.375000\n",
      "Epoch 309, CIFAR-10 Batch 1:  Loss:     1.3770 Validation Accuracy: 0.275000\n",
      "Epoch 310, CIFAR-10 Batch 1:  Loss:     1.4251 Validation Accuracy: 0.350000\n",
      "Epoch 311, CIFAR-10 Batch 1:  Loss:     1.4056 Validation Accuracy: 0.400000\n",
      "Epoch 312, CIFAR-10 Batch 1:  Loss:     1.4214 Validation Accuracy: 0.325000\n",
      "Epoch 313, CIFAR-10 Batch 1:  Loss:     1.4262 Validation Accuracy: 0.375000\n",
      "Epoch 314, CIFAR-10 Batch 1:  Loss:     1.4039 Validation Accuracy: 0.325000\n",
      "Epoch 315, CIFAR-10 Batch 1:  Loss:     1.3957 Validation Accuracy: 0.350000\n",
      "Epoch 316, CIFAR-10 Batch 1:  Loss:     1.3952 Validation Accuracy: 0.375000\n",
      "Epoch 317, CIFAR-10 Batch 1:  Loss:     1.4231 Validation Accuracy: 0.300000\n",
      "Epoch 318, CIFAR-10 Batch 1:  Loss:     1.4093 Validation Accuracy: 0.325000\n",
      "Epoch 319, CIFAR-10 Batch 1:  Loss:     1.3986 Validation Accuracy: 0.325000\n",
      "Epoch 320, CIFAR-10 Batch 1:  Loss:     1.3738 Validation Accuracy: 0.400000\n",
      "Epoch 321, CIFAR-10 Batch 1:  Loss:     1.4055 Validation Accuracy: 0.325000\n",
      "Epoch 322, CIFAR-10 Batch 1:  Loss:     1.3737 Validation Accuracy: 0.400000\n",
      "Epoch 323, CIFAR-10 Batch 1:  Loss:     1.3832 Validation Accuracy: 0.400000\n",
      "Epoch 324, CIFAR-10 Batch 1:  Loss:     1.3662 Validation Accuracy: 0.425000\n",
      "Epoch 325, CIFAR-10 Batch 1:  Loss:     1.3730 Validation Accuracy: 0.350000\n",
      "Epoch 326, CIFAR-10 Batch 1:  Loss:     1.3489 Validation Accuracy: 0.400000\n",
      "Epoch 327, CIFAR-10 Batch 1:  Loss:     1.4227 Validation Accuracy: 0.325000\n",
      "Epoch 328, CIFAR-10 Batch 1:  Loss:     1.3619 Validation Accuracy: 0.400000\n",
      "Epoch 329, CIFAR-10 Batch 1:  Loss:     1.3817 Validation Accuracy: 0.425000\n",
      "Epoch 330, CIFAR-10 Batch 1:  Loss:     1.4289 Validation Accuracy: 0.375000\n",
      "Epoch 331, CIFAR-10 Batch 1:  Loss:     1.4017 Validation Accuracy: 0.400000\n",
      "Epoch 332, CIFAR-10 Batch 1:  Loss:     1.3573 Validation Accuracy: 0.400000\n",
      "Epoch 333, CIFAR-10 Batch 1:  Loss:     1.3581 Validation Accuracy: 0.425000\n",
      "Epoch 334, CIFAR-10 Batch 1:  Loss:     1.4090 Validation Accuracy: 0.425000\n",
      "Epoch 335, CIFAR-10 Batch 1:  Loss:     1.3643 Validation Accuracy: 0.400000\n",
      "Epoch 336, CIFAR-10 Batch 1:  Loss:     1.3375 Validation Accuracy: 0.450000\n",
      "Epoch 337, CIFAR-10 Batch 1:  Loss:     1.3799 Validation Accuracy: 0.450000\n",
      "Epoch 338, CIFAR-10 Batch 1:  Loss:     1.3763 Validation Accuracy: 0.350000\n",
      "Epoch 339, CIFAR-10 Batch 1:  Loss:     1.3782 Validation Accuracy: 0.375000\n",
      "Epoch 340, CIFAR-10 Batch 1:  Loss:     1.3390 Validation Accuracy: 0.475000\n",
      "Epoch 341, CIFAR-10 Batch 1:  Loss:     1.3994 Validation Accuracy: 0.350000\n",
      "Epoch 342, CIFAR-10 Batch 1:  Loss:     1.4121 Validation Accuracy: 0.425000\n",
      "Epoch 343, CIFAR-10 Batch 1:  Loss:     1.3465 Validation Accuracy: 0.375000\n",
      "Epoch 344, CIFAR-10 Batch 1:  Loss:     1.3580 Validation Accuracy: 0.375000\n",
      "Epoch 345, CIFAR-10 Batch 1:  Loss:     1.3865 Validation Accuracy: 0.400000\n",
      "Epoch 346, CIFAR-10 Batch 1:  Loss:     1.4005 Validation Accuracy: 0.425000\n",
      "Epoch 347, CIFAR-10 Batch 1:  Loss:     1.3578 Validation Accuracy: 0.400000\n",
      "Epoch 348, CIFAR-10 Batch 1:  Loss:     1.3752 Validation Accuracy: 0.425000\n",
      "Epoch 349, CIFAR-10 Batch 1:  Loss:     1.3500 Validation Accuracy: 0.425000\n",
      "Epoch 350, CIFAR-10 Batch 1:  Loss:     1.3786 Validation Accuracy: 0.375000\n",
      "Epoch 351, CIFAR-10 Batch 1:  Loss:     1.3436 Validation Accuracy: 0.375000\n",
      "Epoch 352, CIFAR-10 Batch 1:  Loss:     1.3922 Validation Accuracy: 0.400000\n",
      "Epoch 353, CIFAR-10 Batch 1:  Loss:     1.3693 Validation Accuracy: 0.425000\n",
      "Epoch 354, CIFAR-10 Batch 1:  Loss:     1.3840 Validation Accuracy: 0.400000\n",
      "Epoch 355, CIFAR-10 Batch 1:  Loss:     1.3606 Validation Accuracy: 0.400000\n",
      "Epoch 356, CIFAR-10 Batch 1:  Loss:     1.3629 Validation Accuracy: 0.425000\n",
      "Epoch 357, CIFAR-10 Batch 1:  Loss:     1.3269 Validation Accuracy: 0.425000\n",
      "Epoch 358, CIFAR-10 Batch 1:  Loss:     1.3730 Validation Accuracy: 0.450000\n",
      "Epoch 359, CIFAR-10 Batch 1:  Loss:     1.3773 Validation Accuracy: 0.425000\n",
      "Epoch 360, CIFAR-10 Batch 1:  Loss:     1.3519 Validation Accuracy: 0.425000\n",
      "Epoch 361, CIFAR-10 Batch 1:  Loss:     1.3410 Validation Accuracy: 0.450000\n",
      "Epoch 362, CIFAR-10 Batch 1:  Loss:     1.3453 Validation Accuracy: 0.400000\n",
      "Epoch 363, CIFAR-10 Batch 1:  Loss:     1.3578 Validation Accuracy: 0.425000\n",
      "Epoch 364, CIFAR-10 Batch 1:  Loss:     1.3818 Validation Accuracy: 0.400000\n",
      "Epoch 365, CIFAR-10 Batch 1:  Loss:     1.3485 Validation Accuracy: 0.450000\n",
      "Epoch 366, CIFAR-10 Batch 1:  Loss:     1.3411 Validation Accuracy: 0.500000\n",
      "Epoch 367, CIFAR-10 Batch 1:  Loss:     1.3748 Validation Accuracy: 0.450000\n",
      "Epoch 368, CIFAR-10 Batch 1:  Loss:     1.3589 Validation Accuracy: 0.375000\n",
      "Epoch 369, CIFAR-10 Batch 1:  Loss:     1.3809 Validation Accuracy: 0.400000\n",
      "Epoch 370, CIFAR-10 Batch 1:  Loss:     1.3199 Validation Accuracy: 0.450000\n",
      "Epoch 371, CIFAR-10 Batch 1:  Loss:     1.3033 Validation Accuracy: 0.475000\n",
      "Epoch 372, CIFAR-10 Batch 1:  Loss:     1.3945 Validation Accuracy: 0.400000\n",
      "Epoch 373, CIFAR-10 Batch 1:  Loss:     1.3999 Validation Accuracy: 0.475000\n",
      "Epoch 374, CIFAR-10 Batch 1:  Loss:     1.3513 Validation Accuracy: 0.500000\n",
      "Epoch 375, CIFAR-10 Batch 1:  Loss:     1.3426 Validation Accuracy: 0.475000\n",
      "Epoch 376, CIFAR-10 Batch 1:  Loss:     1.4029 Validation Accuracy: 0.400000\n",
      "Epoch 377, CIFAR-10 Batch 1:  Loss:     1.3328 Validation Accuracy: 0.475000\n",
      "Epoch 378, CIFAR-10 Batch 1:  Loss:     1.3590 Validation Accuracy: 0.450000\n",
      "Epoch 379, CIFAR-10 Batch 1:  Loss:     1.3453 Validation Accuracy: 0.475000\n",
      "Epoch 380, CIFAR-10 Batch 1:  Loss:     1.3372 Validation Accuracy: 0.475000\n",
      "Epoch 381, CIFAR-10 Batch 1:  Loss:     1.3602 Validation Accuracy: 0.450000\n",
      "Epoch 382, CIFAR-10 Batch 1:  Loss:     1.3370 Validation Accuracy: 0.500000\n",
      "Epoch 383, CIFAR-10 Batch 1:  Loss:     1.3663 Validation Accuracy: 0.475000\n",
      "Epoch 384, CIFAR-10 Batch 1:  Loss:     1.3777 Validation Accuracy: 0.400000\n",
      "Epoch 385, CIFAR-10 Batch 1:  Loss:     1.3499 Validation Accuracy: 0.400000\n",
      "Epoch 386, CIFAR-10 Batch 1:  Loss:     1.3669 Validation Accuracy: 0.400000\n",
      "Epoch 387, CIFAR-10 Batch 1:  Loss:     1.3505 Validation Accuracy: 0.450000\n",
      "Epoch 388, CIFAR-10 Batch 1:  Loss:     1.3743 Validation Accuracy: 0.375000\n",
      "Epoch 389, CIFAR-10 Batch 1:  Loss:     1.3466 Validation Accuracy: 0.425000\n",
      "Epoch 390, CIFAR-10 Batch 1:  Loss:     1.3870 Validation Accuracy: 0.425000\n",
      "Epoch 391, CIFAR-10 Batch 1:  Loss:     1.3535 Validation Accuracy: 0.400000\n",
      "Epoch 392, CIFAR-10 Batch 1:  Loss:     1.3458 Validation Accuracy: 0.450000\n",
      "Epoch 393, CIFAR-10 Batch 1:  Loss:     1.3980 Validation Accuracy: 0.425000\n",
      "Epoch 394, CIFAR-10 Batch 1:  Loss:     1.3445 Validation Accuracy: 0.425000\n",
      "Epoch 395, CIFAR-10 Batch 1:  Loss:     1.3340 Validation Accuracy: 0.475000\n",
      "Epoch 396, CIFAR-10 Batch 1:  Loss:     1.3062 Validation Accuracy: 0.450000\n",
      "Epoch 397, CIFAR-10 Batch 1:  Loss:     1.3356 Validation Accuracy: 0.500000\n",
      "Epoch 398, CIFAR-10 Batch 1:  Loss:     1.3208 Validation Accuracy: 0.475000\n",
      "Epoch 399, CIFAR-10 Batch 1:  Loss:     1.3636 Validation Accuracy: 0.450000\n",
      "Epoch 400, CIFAR-10 Batch 1:  Loss:     1.3172 Validation Accuracy: 0.450000\n",
      "Epoch 401, CIFAR-10 Batch 1:  Loss:     1.3470 Validation Accuracy: 0.450000\n",
      "Epoch 402, CIFAR-10 Batch 1:  Loss:     1.3279 Validation Accuracy: 0.500000\n",
      "Epoch 403, CIFAR-10 Batch 1:  Loss:     1.3568 Validation Accuracy: 0.425000\n",
      "Epoch 404, CIFAR-10 Batch 1:  Loss:     1.3220 Validation Accuracy: 0.425000\n",
      "Epoch 405, CIFAR-10 Batch 1:  Loss:     1.3510 Validation Accuracy: 0.425000\n",
      "Epoch 406, CIFAR-10 Batch 1:  Loss:     1.3370 Validation Accuracy: 0.425000\n",
      "Epoch 407, CIFAR-10 Batch 1:  Loss:     1.3366 Validation Accuracy: 0.475000\n",
      "Epoch 408, CIFAR-10 Batch 1:  Loss:     1.3284 Validation Accuracy: 0.475000\n",
      "Epoch 409, CIFAR-10 Batch 1:  Loss:     1.3615 Validation Accuracy: 0.425000\n",
      "Epoch 410, CIFAR-10 Batch 1:  Loss:     1.3566 Validation Accuracy: 0.450000\n",
      "Epoch 411, CIFAR-10 Batch 1:  Loss:     1.3158 Validation Accuracy: 0.450000\n",
      "Epoch 412, CIFAR-10 Batch 1:  Loss:     1.3575 Validation Accuracy: 0.425000\n",
      "Epoch 413, CIFAR-10 Batch 1:  Loss:     1.3311 Validation Accuracy: 0.400000\n",
      "Epoch 414, CIFAR-10 Batch 1:  Loss:     1.3589 Validation Accuracy: 0.425000\n",
      "Epoch 415, CIFAR-10 Batch 1:  Loss:     1.3605 Validation Accuracy: 0.425000\n",
      "Epoch 416, CIFAR-10 Batch 1:  Loss:     1.3704 Validation Accuracy: 0.400000\n",
      "Epoch 417, CIFAR-10 Batch 1:  Loss:     1.3695 Validation Accuracy: 0.425000\n",
      "Epoch 418, CIFAR-10 Batch 1:  Loss:     1.3247 Validation Accuracy: 0.450000\n",
      "Epoch 419, CIFAR-10 Batch 1:  Loss:     1.3327 Validation Accuracy: 0.500000\n",
      "Epoch 420, CIFAR-10 Batch 1:  Loss:     1.3488 Validation Accuracy: 0.500000\n",
      "Epoch 421, CIFAR-10 Batch 1:  Loss:     1.3756 Validation Accuracy: 0.450000\n",
      "Epoch 422, CIFAR-10 Batch 1:  Loss:     1.3174 Validation Accuracy: 0.450000\n",
      "Epoch 423, CIFAR-10 Batch 1:  Loss:     1.3075 Validation Accuracy: 0.500000\n",
      "Epoch 424, CIFAR-10 Batch 1:  Loss:     1.3523 Validation Accuracy: 0.525000\n",
      "Epoch 425, CIFAR-10 Batch 1:  Loss:     1.2978 Validation Accuracy: 0.550000\n",
      "Epoch 426, CIFAR-10 Batch 1:  Loss:     1.2982 Validation Accuracy: 0.550000\n",
      "Epoch 427, CIFAR-10 Batch 1:  Loss:     1.3576 Validation Accuracy: 0.525000\n",
      "Epoch 428, CIFAR-10 Batch 1:  Loss:     1.4268 Validation Accuracy: 0.450000\n",
      "Epoch 429, CIFAR-10 Batch 1:  Loss:     1.3207 Validation Accuracy: 0.475000\n",
      "Epoch 430, CIFAR-10 Batch 1:  Loss:     1.3219 Validation Accuracy: 0.500000\n",
      "Epoch 431, CIFAR-10 Batch 1:  Loss:     1.3175 Validation Accuracy: 0.500000\n",
      "Epoch 432, CIFAR-10 Batch 1:  Loss:     1.3344 Validation Accuracy: 0.500000\n",
      "Epoch 433, CIFAR-10 Batch 1:  Loss:     1.3197 Validation Accuracy: 0.475000\n",
      "Epoch 434, CIFAR-10 Batch 1:  Loss:     1.3215 Validation Accuracy: 0.525000\n",
      "Epoch 435, CIFAR-10 Batch 1:  Loss:     1.3436 Validation Accuracy: 0.475000\n",
      "Epoch 436, CIFAR-10 Batch 1:  Loss:     1.3256 Validation Accuracy: 0.475000\n",
      "Epoch 437, CIFAR-10 Batch 1:  Loss:     1.3203 Validation Accuracy: 0.475000\n",
      "Epoch 438, CIFAR-10 Batch 1:  Loss:     1.2727 Validation Accuracy: 0.475000\n",
      "Epoch 439, CIFAR-10 Batch 1:  Loss:     1.2788 Validation Accuracy: 0.475000\n",
      "Epoch 440, CIFAR-10 Batch 1:  Loss:     1.3176 Validation Accuracy: 0.475000\n",
      "Epoch 441, CIFAR-10 Batch 1:  Loss:     1.3305 Validation Accuracy: 0.425000\n",
      "Epoch 442, CIFAR-10 Batch 1:  Loss:     1.3243 Validation Accuracy: 0.450000\n",
      "Epoch 443, CIFAR-10 Batch 1:  Loss:     1.3840 Validation Accuracy: 0.450000\n",
      "Epoch 444, CIFAR-10 Batch 1:  Loss:     1.3328 Validation Accuracy: 0.450000\n",
      "Epoch 445, CIFAR-10 Batch 1:  Loss:     1.3249 Validation Accuracy: 0.450000\n",
      "Epoch 446, CIFAR-10 Batch 1:  Loss:     1.3161 Validation Accuracy: 0.425000\n",
      "Epoch 447, CIFAR-10 Batch 1:  Loss:     1.2993 Validation Accuracy: 0.475000\n",
      "Epoch 448, CIFAR-10 Batch 1:  Loss:     1.3157 Validation Accuracy: 0.500000\n",
      "Epoch 449, CIFAR-10 Batch 1:  Loss:     1.3386 Validation Accuracy: 0.450000\n",
      "Epoch 450, CIFAR-10 Batch 1:  Loss:     1.3344 Validation Accuracy: 0.475000\n",
      "Epoch 451, CIFAR-10 Batch 1:  Loss:     1.3259 Validation Accuracy: 0.450000\n",
      "Epoch 452, CIFAR-10 Batch 1:  Loss:     1.3338 Validation Accuracy: 0.450000\n",
      "Epoch 453, CIFAR-10 Batch 1:  Loss:     1.3434 Validation Accuracy: 0.475000\n",
      "Epoch 454, CIFAR-10 Batch 1:  Loss:     1.3408 Validation Accuracy: 0.450000\n",
      "Epoch 455, CIFAR-10 Batch 1:  Loss:     1.3274 Validation Accuracy: 0.400000\n",
      "Epoch 456, CIFAR-10 Batch 1:  Loss:     1.3251 Validation Accuracy: 0.500000\n",
      "Epoch 457, CIFAR-10 Batch 1:  Loss:     1.2723 Validation Accuracy: 0.550000\n",
      "Epoch 458, CIFAR-10 Batch 1:  Loss:     1.3194 Validation Accuracy: 0.525000\n",
      "Epoch 459, CIFAR-10 Batch 1:  Loss:     1.3006 Validation Accuracy: 0.500000\n",
      "Epoch 460, CIFAR-10 Batch 1:  Loss:     1.3274 Validation Accuracy: 0.475000\n",
      "Epoch 461, CIFAR-10 Batch 1:  Loss:     1.2909 Validation Accuracy: 0.475000\n",
      "Epoch 462, CIFAR-10 Batch 1:  Loss:     1.3315 Validation Accuracy: 0.475000\n",
      "Epoch 463, CIFAR-10 Batch 1:  Loss:     1.3223 Validation Accuracy: 0.475000\n",
      "Epoch 464, CIFAR-10 Batch 1:  Loss:     1.3215 Validation Accuracy: 0.425000\n",
      "Epoch 465, CIFAR-10 Batch 1:  Loss:     1.3507 Validation Accuracy: 0.450000\n",
      "Epoch 466, CIFAR-10 Batch 1:  Loss:     1.2942 Validation Accuracy: 0.425000\n",
      "Epoch 467, CIFAR-10 Batch 1:  Loss:     1.3152 Validation Accuracy: 0.450000\n",
      "Epoch 468, CIFAR-10 Batch 1:  Loss:     1.3333 Validation Accuracy: 0.400000\n",
      "Epoch 469, CIFAR-10 Batch 1:  Loss:     1.3484 Validation Accuracy: 0.425000\n",
      "Epoch 470, CIFAR-10 Batch 1:  Loss:     1.3046 Validation Accuracy: 0.475000\n",
      "Epoch 471, CIFAR-10 Batch 1:  Loss:     1.3376 Validation Accuracy: 0.500000\n",
      "Epoch 472, CIFAR-10 Batch 1:  Loss:     1.3119 Validation Accuracy: 0.475000\n",
      "Epoch 473, CIFAR-10 Batch 1:  Loss:     1.2977 Validation Accuracy: 0.475000\n",
      "Epoch 474, CIFAR-10 Batch 1:  Loss:     1.2981 Validation Accuracy: 0.525000\n",
      "Epoch 475, CIFAR-10 Batch 1:  Loss:     1.3155 Validation Accuracy: 0.525000\n",
      "Epoch 476, CIFAR-10 Batch 1:  Loss:     1.3637 Validation Accuracy: 0.450000\n",
      "Epoch 477, CIFAR-10 Batch 1:  Loss:     1.2684 Validation Accuracy: 0.550000\n",
      "Epoch 478, CIFAR-10 Batch 1:  Loss:     1.3264 Validation Accuracy: 0.425000\n",
      "Epoch 479, CIFAR-10 Batch 1:  Loss:     1.2999 Validation Accuracy: 0.475000\n",
      "Epoch 480, CIFAR-10 Batch 1:  Loss:     1.3220 Validation Accuracy: 0.425000\n",
      "Epoch 481, CIFAR-10 Batch 1:  Loss:     1.3274 Validation Accuracy: 0.400000\n",
      "Epoch 482, CIFAR-10 Batch 1:  Loss:     1.3125 Validation Accuracy: 0.400000\n",
      "Epoch 483, CIFAR-10 Batch 1:  Loss:     1.3158 Validation Accuracy: 0.375000\n",
      "Epoch 484, CIFAR-10 Batch 1:  Loss:     1.3059 Validation Accuracy: 0.400000\n",
      "Epoch 485, CIFAR-10 Batch 1:  Loss:     1.3123 Validation Accuracy: 0.450000\n",
      "Epoch 486, CIFAR-10 Batch 1:  Loss:     1.3487 Validation Accuracy: 0.450000\n",
      "Epoch 487, CIFAR-10 Batch 1:  Loss:     1.2965 Validation Accuracy: 0.425000\n",
      "Epoch 488, CIFAR-10 Batch 1:  Loss:     1.3078 Validation Accuracy: 0.425000\n",
      "Epoch 489, CIFAR-10 Batch 1:  Loss:     1.3387 Validation Accuracy: 0.400000\n",
      "Epoch 490, CIFAR-10 Batch 1:  Loss:     1.3093 Validation Accuracy: 0.425000\n",
      "Epoch 491, CIFAR-10 Batch 1:  Loss:     1.3146 Validation Accuracy: 0.400000\n",
      "Epoch 492, CIFAR-10 Batch 1:  Loss:     1.3065 Validation Accuracy: 0.425000\n",
      "Epoch 493, CIFAR-10 Batch 1:  Loss:     1.3449 Validation Accuracy: 0.375000\n",
      "Epoch 494, CIFAR-10 Batch 1:  Loss:     1.3234 Validation Accuracy: 0.400000\n",
      "Epoch 495, CIFAR-10 Batch 1:  Loss:     1.2748 Validation Accuracy: 0.475000\n",
      "Epoch 496, CIFAR-10 Batch 1:  Loss:     1.3067 Validation Accuracy: 0.475000\n",
      "Epoch 497, CIFAR-10 Batch 1:  Loss:     1.3051 Validation Accuracy: 0.450000\n",
      "Epoch 498, CIFAR-10 Batch 1:  Loss:     1.3149 Validation Accuracy: 0.450000\n",
      "Epoch 499, CIFAR-10 Batch 1:  Loss:     1.3293 Validation Accuracy: 0.400000\n",
      "Epoch 500, CIFAR-10 Batch 1:  Loss:     1.3252 Validation Accuracy: 0.425000\n",
      "Epoch 501, CIFAR-10 Batch 1:  Loss:     1.2927 Validation Accuracy: 0.475000\n",
      "Epoch 502, CIFAR-10 Batch 1:  Loss:     1.3284 Validation Accuracy: 0.425000\n",
      "Epoch 503, CIFAR-10 Batch 1:  Loss:     1.3460 Validation Accuracy: 0.350000\n",
      "Epoch 504, CIFAR-10 Batch 1:  Loss:     1.3069 Validation Accuracy: 0.450000\n",
      "Epoch 505, CIFAR-10 Batch 1:  Loss:     1.3423 Validation Accuracy: 0.425000\n",
      "Epoch 506, CIFAR-10 Batch 1:  Loss:     1.3304 Validation Accuracy: 0.400000\n",
      "Epoch 507, CIFAR-10 Batch 1:  Loss:     1.3237 Validation Accuracy: 0.450000\n",
      "Epoch 508, CIFAR-10 Batch 1:  Loss:     1.3279 Validation Accuracy: 0.500000\n",
      "Epoch 509, CIFAR-10 Batch 1:  Loss:     1.3170 Validation Accuracy: 0.475000\n",
      "Epoch 510, CIFAR-10 Batch 1:  Loss:     1.3082 Validation Accuracy: 0.475000\n",
      "Epoch 511, CIFAR-10 Batch 1:  Loss:     1.3082 Validation Accuracy: 0.500000\n",
      "Epoch 512, CIFAR-10 Batch 1:  Loss:     1.2934 Validation Accuracy: 0.500000\n",
      "Epoch 513, CIFAR-10 Batch 1:  Loss:     1.3252 Validation Accuracy: 0.425000\n",
      "Epoch 514, CIFAR-10 Batch 1:  Loss:     1.2973 Validation Accuracy: 0.450000\n",
      "Epoch 515, CIFAR-10 Batch 1:  Loss:     1.3624 Validation Accuracy: 0.425000\n",
      "Epoch 516, CIFAR-10 Batch 1:  Loss:     1.2686 Validation Accuracy: 0.475000\n",
      "Epoch 517, CIFAR-10 Batch 1:  Loss:     1.3265 Validation Accuracy: 0.425000\n",
      "Epoch 518, CIFAR-10 Batch 1:  Loss:     1.3200 Validation Accuracy: 0.425000\n",
      "Epoch 519, CIFAR-10 Batch 1:  Loss:     1.3963 Validation Accuracy: 0.425000\n",
      "Epoch 520, CIFAR-10 Batch 1:  Loss:     1.3204 Validation Accuracy: 0.500000\n",
      "Epoch 521, CIFAR-10 Batch 1:  Loss:     1.3484 Validation Accuracy: 0.425000\n",
      "Epoch 522, CIFAR-10 Batch 1:  Loss:     1.2793 Validation Accuracy: 0.475000\n",
      "Epoch 523, CIFAR-10 Batch 1:  Loss:     1.3711 Validation Accuracy: 0.450000\n",
      "Epoch 524, CIFAR-10 Batch 1:  Loss:     1.2805 Validation Accuracy: 0.475000\n",
      "Epoch 525, CIFAR-10 Batch 1:  Loss:     1.3077 Validation Accuracy: 0.450000\n",
      "Epoch 526, CIFAR-10 Batch 1:  Loss:     1.3187 Validation Accuracy: 0.425000\n",
      "Epoch 527, CIFAR-10 Batch 1:  Loss:     1.2803 Validation Accuracy: 0.525000\n",
      "Epoch 528, CIFAR-10 Batch 1:  Loss:     1.2945 Validation Accuracy: 0.525000\n",
      "Epoch 529, CIFAR-10 Batch 1:  Loss:     1.2961 Validation Accuracy: 0.450000\n",
      "Epoch 530, CIFAR-10 Batch 1:  Loss:     1.2819 Validation Accuracy: 0.525000\n",
      "Epoch 531, CIFAR-10 Batch 1:  Loss:     1.3582 Validation Accuracy: 0.450000\n",
      "Epoch 532, CIFAR-10 Batch 1:  Loss:     1.2560 Validation Accuracy: 0.550000\n",
      "Epoch 533, CIFAR-10 Batch 1:  Loss:     1.2956 Validation Accuracy: 0.525000\n",
      "Epoch 534, CIFAR-10 Batch 1:  Loss:     1.3220 Validation Accuracy: 0.500000\n",
      "Epoch 535, CIFAR-10 Batch 1:  Loss:     1.3132 Validation Accuracy: 0.450000\n",
      "Epoch 536, CIFAR-10 Batch 1:  Loss:     1.2641 Validation Accuracy: 0.525000\n",
      "Epoch 537, CIFAR-10 Batch 1:  Loss:     1.3166 Validation Accuracy: 0.475000\n",
      "Epoch 538, CIFAR-10 Batch 1:  Loss:     1.2734 Validation Accuracy: 0.525000\n",
      "Epoch 539, CIFAR-10 Batch 1:  Loss:     1.3747 Validation Accuracy: 0.450000\n",
      "Epoch 540, CIFAR-10 Batch 1:  Loss:     1.2878 Validation Accuracy: 0.475000\n",
      "Epoch 541, CIFAR-10 Batch 1:  Loss:     1.3241 Validation Accuracy: 0.425000\n",
      "Epoch 542, CIFAR-10 Batch 1:  Loss:     1.3095 Validation Accuracy: 0.475000\n",
      "Epoch 543, CIFAR-10 Batch 1:  Loss:     1.2784 Validation Accuracy: 0.500000\n",
      "Epoch 544, CIFAR-10 Batch 1:  Loss:     1.2601 Validation Accuracy: 0.575000\n",
      "Epoch 545, CIFAR-10 Batch 1:  Loss:     1.3142 Validation Accuracy: 0.500000\n",
      "Epoch 546, CIFAR-10 Batch 1:  Loss:     1.2713 Validation Accuracy: 0.475000\n",
      "Epoch 547, CIFAR-10 Batch 1:  Loss:     1.2653 Validation Accuracy: 0.575000\n",
      "Epoch 548, CIFAR-10 Batch 1:  Loss:     1.3392 Validation Accuracy: 0.450000\n",
      "Epoch 549, CIFAR-10 Batch 1:  Loss:     1.3297 Validation Accuracy: 0.450000\n",
      "Epoch 550, CIFAR-10 Batch 1:  Loss:     1.3293 Validation Accuracy: 0.450000\n",
      "Epoch 551, CIFAR-10 Batch 1:  Loss:     1.2633 Validation Accuracy: 0.550000\n",
      "Epoch 552, CIFAR-10 Batch 1:  Loss:     1.2837 Validation Accuracy: 0.475000\n",
      "Epoch 553, CIFAR-10 Batch 1:  Loss:     1.2408 Validation Accuracy: 0.525000\n",
      "Epoch 554, CIFAR-10 Batch 1:  Loss:     1.3087 Validation Accuracy: 0.475000\n",
      "Epoch 555, CIFAR-10 Batch 1:  Loss:     1.2891 Validation Accuracy: 0.525000\n",
      "Epoch 556, CIFAR-10 Batch 1:  Loss:     1.2731 Validation Accuracy: 0.550000\n",
      "Epoch 557, CIFAR-10 Batch 1:  Loss:     1.2518 Validation Accuracy: 0.500000\n",
      "Epoch 558, CIFAR-10 Batch 1:  Loss:     1.2657 Validation Accuracy: 0.500000\n",
      "Epoch 559, CIFAR-10 Batch 1:  Loss:     1.3887 Validation Accuracy: 0.500000\n",
      "Epoch 560, CIFAR-10 Batch 1:  Loss:     1.3046 Validation Accuracy: 0.500000\n",
      "Epoch 561, CIFAR-10 Batch 1:  Loss:     1.3126 Validation Accuracy: 0.475000\n",
      "Epoch 562, CIFAR-10 Batch 1:  Loss:     1.3361 Validation Accuracy: 0.500000\n",
      "Epoch 563, CIFAR-10 Batch 1:  Loss:     1.3003 Validation Accuracy: 0.500000\n",
      "Epoch 564, CIFAR-10 Batch 1:  Loss:     1.2612 Validation Accuracy: 0.525000\n",
      "Epoch 565, CIFAR-10 Batch 1:  Loss:     1.3100 Validation Accuracy: 0.475000\n",
      "Epoch 566, CIFAR-10 Batch 1:  Loss:     1.3121 Validation Accuracy: 0.475000\n",
      "Epoch 567, CIFAR-10 Batch 1:  Loss:     1.2922 Validation Accuracy: 0.450000\n",
      "Epoch 568, CIFAR-10 Batch 1:  Loss:     1.2961 Validation Accuracy: 0.450000\n",
      "Epoch 569, CIFAR-10 Batch 1:  Loss:     1.3183 Validation Accuracy: 0.475000\n",
      "Epoch 570, CIFAR-10 Batch 1:  Loss:     1.2705 Validation Accuracy: 0.500000\n",
      "Epoch 571, CIFAR-10 Batch 1:  Loss:     1.2678 Validation Accuracy: 0.475000\n",
      "Epoch 572, CIFAR-10 Batch 1:  Loss:     1.3103 Validation Accuracy: 0.475000\n",
      "Epoch 573, CIFAR-10 Batch 1:  Loss:     1.2602 Validation Accuracy: 0.500000\n",
      "Epoch 574, CIFAR-10 Batch 1:  Loss:     1.2915 Validation Accuracy: 0.475000\n",
      "Epoch 575, CIFAR-10 Batch 1:  Loss:     1.2515 Validation Accuracy: 0.525000\n",
      "Epoch 576, CIFAR-10 Batch 1:  Loss:     1.3245 Validation Accuracy: 0.450000\n",
      "Epoch 577, CIFAR-10 Batch 1:  Loss:     1.2712 Validation Accuracy: 0.500000\n",
      "Epoch 578, CIFAR-10 Batch 1:  Loss:     1.2832 Validation Accuracy: 0.475000\n",
      "Epoch 579, CIFAR-10 Batch 1:  Loss:     1.3088 Validation Accuracy: 0.550000\n",
      "Epoch 580, CIFAR-10 Batch 1:  Loss:     1.2623 Validation Accuracy: 0.500000\n",
      "Epoch 581, CIFAR-10 Batch 1:  Loss:     1.3097 Validation Accuracy: 0.475000\n",
      "Epoch 582, CIFAR-10 Batch 1:  Loss:     1.2631 Validation Accuracy: 0.550000\n",
      "Epoch 583, CIFAR-10 Batch 1:  Loss:     1.2467 Validation Accuracy: 0.500000\n",
      "Epoch 584, CIFAR-10 Batch 1:  Loss:     1.3026 Validation Accuracy: 0.450000\n",
      "Epoch 585, CIFAR-10 Batch 1:  Loss:     1.3089 Validation Accuracy: 0.450000\n",
      "Epoch 586, CIFAR-10 Batch 1:  Loss:     1.3163 Validation Accuracy: 0.475000\n",
      "Epoch 587, CIFAR-10 Batch 1:  Loss:     1.3363 Validation Accuracy: 0.425000\n",
      "Epoch 588, CIFAR-10 Batch 1:  Loss:     1.2855 Validation Accuracy: 0.475000\n",
      "Epoch 589, CIFAR-10 Batch 1:  Loss:     1.3376 Validation Accuracy: 0.425000\n",
      "Epoch 590, CIFAR-10 Batch 1:  Loss:     1.3595 Validation Accuracy: 0.450000\n",
      "Epoch 591, CIFAR-10 Batch 1:  Loss:     1.3253 Validation Accuracy: 0.425000\n",
      "Epoch 592, CIFAR-10 Batch 1:  Loss:     1.2882 Validation Accuracy: 0.550000\n",
      "Epoch 593, CIFAR-10 Batch 1:  Loss:     1.3268 Validation Accuracy: 0.475000\n",
      "Epoch 594, CIFAR-10 Batch 1:  Loss:     1.2685 Validation Accuracy: 0.525000\n",
      "Epoch 595, CIFAR-10 Batch 1:  Loss:     1.3600 Validation Accuracy: 0.450000\n",
      "Epoch 596, CIFAR-10 Batch 1:  Loss:     1.2835 Validation Accuracy: 0.525000\n",
      "Epoch 597, CIFAR-10 Batch 1:  Loss:     1.2489 Validation Accuracy: 0.575000\n",
      "Epoch 598, CIFAR-10 Batch 1:  Loss:     1.3061 Validation Accuracy: 0.475000\n",
      "Epoch 599, CIFAR-10 Batch 1:  Loss:     1.3157 Validation Accuracy: 0.425000\n",
      "Epoch 600, CIFAR-10 Batch 1:  Loss:     1.3316 Validation Accuracy: 0.475000\n",
      "Epoch 601, CIFAR-10 Batch 1:  Loss:     1.3623 Validation Accuracy: 0.475000\n",
      "Epoch 602, CIFAR-10 Batch 1:  Loss:     1.2950 Validation Accuracy: 0.500000\n",
      "Epoch 603, CIFAR-10 Batch 1:  Loss:     1.2957 Validation Accuracy: 0.500000\n",
      "Epoch 604, CIFAR-10 Batch 1:  Loss:     1.2765 Validation Accuracy: 0.525000\n",
      "Epoch 605, CIFAR-10 Batch 1:  Loss:     1.2873 Validation Accuracy: 0.575000\n",
      "Epoch 606, CIFAR-10 Batch 1:  Loss:     1.2847 Validation Accuracy: 0.525000\n",
      "Epoch 607, CIFAR-10 Batch 1:  Loss:     1.2540 Validation Accuracy: 0.550000\n",
      "Epoch 608, CIFAR-10 Batch 1:  Loss:     1.2935 Validation Accuracy: 0.525000\n",
      "Epoch 609, CIFAR-10 Batch 1:  Loss:     1.2736 Validation Accuracy: 0.550000\n",
      "Epoch 610, CIFAR-10 Batch 1:  Loss:     1.3325 Validation Accuracy: 0.500000\n",
      "Epoch 611, CIFAR-10 Batch 1:  Loss:     1.3895 Validation Accuracy: 0.450000\n",
      "Epoch 612, CIFAR-10 Batch 1:  Loss:     1.2752 Validation Accuracy: 0.500000\n",
      "Epoch 613, CIFAR-10 Batch 1:  Loss:     1.2952 Validation Accuracy: 0.525000\n",
      "Epoch 614, CIFAR-10 Batch 1:  Loss:     1.3109 Validation Accuracy: 0.475000\n",
      "Epoch 615, CIFAR-10 Batch 1:  Loss:     1.2472 Validation Accuracy: 0.550000\n",
      "Epoch 616, CIFAR-10 Batch 1:  Loss:     1.2394 Validation Accuracy: 0.550000\n",
      "Epoch 617, CIFAR-10 Batch 1:  Loss:     1.3127 Validation Accuracy: 0.550000\n",
      "Epoch 618, CIFAR-10 Batch 1:  Loss:     1.3003 Validation Accuracy: 0.525000\n",
      "Epoch 619, CIFAR-10 Batch 1:  Loss:     1.3019 Validation Accuracy: 0.500000\n",
      "Epoch 620, CIFAR-10 Batch 1:  Loss:     1.3717 Validation Accuracy: 0.500000\n",
      "Epoch 621, CIFAR-10 Batch 1:  Loss:     1.3214 Validation Accuracy: 0.525000\n",
      "Epoch 622, CIFAR-10 Batch 1:  Loss:     1.2714 Validation Accuracy: 0.525000\n",
      "Epoch 623, CIFAR-10 Batch 1:  Loss:     1.2846 Validation Accuracy: 0.525000\n",
      "Epoch 624, CIFAR-10 Batch 1:  Loss:     1.2510 Validation Accuracy: 0.550000\n",
      "Epoch 625, CIFAR-10 Batch 1:  Loss:     1.2785 Validation Accuracy: 0.550000\n",
      "Epoch 626, CIFAR-10 Batch 1:  Loss:     1.2859 Validation Accuracy: 0.475000\n",
      "Epoch 627, CIFAR-10 Batch 1:  Loss:     1.3083 Validation Accuracy: 0.525000\n",
      "Epoch 628, CIFAR-10 Batch 1:  Loss:     1.2808 Validation Accuracy: 0.500000\n",
      "Epoch 629, CIFAR-10 Batch 1:  Loss:     1.2825 Validation Accuracy: 0.550000\n",
      "Epoch 630, CIFAR-10 Batch 1:  Loss:     1.2960 Validation Accuracy: 0.475000\n",
      "Epoch 631, CIFAR-10 Batch 1:  Loss:     1.2892 Validation Accuracy: 0.500000\n",
      "Epoch 632, CIFAR-10 Batch 1:  Loss:     1.2986 Validation Accuracy: 0.450000\n",
      "Epoch 633, CIFAR-10 Batch 1:  Loss:     1.2452 Validation Accuracy: 0.475000\n",
      "Epoch 634, CIFAR-10 Batch 1:  Loss:     1.3316 Validation Accuracy: 0.450000\n",
      "Epoch 635, CIFAR-10 Batch 1:  Loss:     1.2324 Validation Accuracy: 0.525000\n",
      "Epoch 636, CIFAR-10 Batch 1:  Loss:     1.2559 Validation Accuracy: 0.500000\n",
      "Epoch 637, CIFAR-10 Batch 1:  Loss:     1.2638 Validation Accuracy: 0.500000\n",
      "Epoch 638, CIFAR-10 Batch 1:  Loss:     1.2694 Validation Accuracy: 0.475000\n",
      "Epoch 639, CIFAR-10 Batch 1:  Loss:     1.3495 Validation Accuracy: 0.450000\n",
      "Epoch 640, CIFAR-10 Batch 1:  Loss:     1.3122 Validation Accuracy: 0.425000\n",
      "Epoch 641, CIFAR-10 Batch 1:  Loss:     1.3076 Validation Accuracy: 0.500000\n",
      "Epoch 642, CIFAR-10 Batch 1:  Loss:     1.2388 Validation Accuracy: 0.525000\n",
      "Epoch 643, CIFAR-10 Batch 1:  Loss:     1.2863 Validation Accuracy: 0.525000\n",
      "Epoch 644, CIFAR-10 Batch 1:  Loss:     1.3103 Validation Accuracy: 0.500000\n",
      "Epoch 645, CIFAR-10 Batch 1:  Loss:     1.2656 Validation Accuracy: 0.525000\n",
      "Epoch 646, CIFAR-10 Batch 1:  Loss:     1.2421 Validation Accuracy: 0.600000\n",
      "Epoch 647, CIFAR-10 Batch 1:  Loss:     1.2721 Validation Accuracy: 0.550000\n",
      "Epoch 648, CIFAR-10 Batch 1:  Loss:     1.2903 Validation Accuracy: 0.550000\n",
      "Epoch 649, CIFAR-10 Batch 1:  Loss:     1.3447 Validation Accuracy: 0.500000\n",
      "Epoch 650, CIFAR-10 Batch 1:  Loss:     1.2338 Validation Accuracy: 0.600000\n",
      "Epoch 651, CIFAR-10 Batch 1:  Loss:     1.2914 Validation Accuracy: 0.550000\n",
      "Epoch 652, CIFAR-10 Batch 1:  Loss:     1.2923 Validation Accuracy: 0.500000\n",
      "Epoch 653, CIFAR-10 Batch 1:  Loss:     1.3410 Validation Accuracy: 0.550000\n",
      "Epoch 654, CIFAR-10 Batch 1:  Loss:     1.2924 Validation Accuracy: 0.550000\n",
      "Epoch 655, CIFAR-10 Batch 1:  Loss:     1.2953 Validation Accuracy: 0.525000\n",
      "Epoch 656, CIFAR-10 Batch 1:  Loss:     1.2678 Validation Accuracy: 0.475000\n",
      "Epoch 657, CIFAR-10 Batch 1:  Loss:     1.2589 Validation Accuracy: 0.500000\n",
      "Epoch 658, CIFAR-10 Batch 1:  Loss:     1.2649 Validation Accuracy: 0.525000\n",
      "Epoch 659, CIFAR-10 Batch 1:  Loss:     1.2749 Validation Accuracy: 0.550000\n",
      "Epoch 660, CIFAR-10 Batch 1:  Loss:     1.2768 Validation Accuracy: 0.550000\n",
      "Epoch 661, CIFAR-10 Batch 1:  Loss:     1.2937 Validation Accuracy: 0.500000\n",
      "Epoch 662, CIFAR-10 Batch 1:  Loss:     1.2469 Validation Accuracy: 0.525000\n",
      "Epoch 663, CIFAR-10 Batch 1:  Loss:     1.3194 Validation Accuracy: 0.500000\n",
      "Epoch 664, CIFAR-10 Batch 1:  Loss:     1.2794 Validation Accuracy: 0.500000\n",
      "Epoch 665, CIFAR-10 Batch 1:  Loss:     1.2780 Validation Accuracy: 0.525000\n",
      "Epoch 666, CIFAR-10 Batch 1:  Loss:     1.2821 Validation Accuracy: 0.450000\n",
      "Epoch 667, CIFAR-10 Batch 1:  Loss:     1.2204 Validation Accuracy: 0.525000\n",
      "Epoch 668, CIFAR-10 Batch 1:  Loss:     1.2360 Validation Accuracy: 0.575000\n",
      "Epoch 669, CIFAR-10 Batch 1:  Loss:     1.2485 Validation Accuracy: 0.525000\n",
      "Epoch 670, CIFAR-10 Batch 1:  Loss:     1.3095 Validation Accuracy: 0.500000\n",
      "Epoch 671, CIFAR-10 Batch 1:  Loss:     1.2980 Validation Accuracy: 0.500000\n",
      "Epoch 672, CIFAR-10 Batch 1:  Loss:     1.2470 Validation Accuracy: 0.475000\n",
      "Epoch 673, CIFAR-10 Batch 1:  Loss:     1.2903 Validation Accuracy: 0.500000\n",
      "Epoch 674, CIFAR-10 Batch 1:  Loss:     1.2750 Validation Accuracy: 0.525000\n",
      "Epoch 675, CIFAR-10 Batch 1:  Loss:     1.2946 Validation Accuracy: 0.475000\n",
      "Epoch 676, CIFAR-10 Batch 1:  Loss:     1.2696 Validation Accuracy: 0.525000\n",
      "Epoch 677, CIFAR-10 Batch 1:  Loss:     1.3109 Validation Accuracy: 0.500000\n",
      "Epoch 678, CIFAR-10 Batch 1:  Loss:     1.2806 Validation Accuracy: 0.600000\n",
      "Epoch 679, CIFAR-10 Batch 1:  Loss:     1.2835 Validation Accuracy: 0.575000\n",
      "Epoch 680, CIFAR-10 Batch 1:  Loss:     1.2570 Validation Accuracy: 0.550000\n",
      "Epoch 681, CIFAR-10 Batch 1:  Loss:     1.2828 Validation Accuracy: 0.475000\n",
      "Epoch 682, CIFAR-10 Batch 1:  Loss:     1.2624 Validation Accuracy: 0.550000\n",
      "Epoch 683, CIFAR-10 Batch 1:  Loss:     1.2893 Validation Accuracy: 0.550000\n",
      "Epoch 684, CIFAR-10 Batch 1:  Loss:     1.2463 Validation Accuracy: 0.575000\n",
      "Epoch 685, CIFAR-10 Batch 1:  Loss:     1.3325 Validation Accuracy: 0.500000\n",
      "Epoch 686, CIFAR-10 Batch 1:  Loss:     1.2230 Validation Accuracy: 0.575000\n",
      "Epoch 687, CIFAR-10 Batch 1:  Loss:     1.2833 Validation Accuracy: 0.525000\n",
      "Epoch 688, CIFAR-10 Batch 1:  Loss:     1.2254 Validation Accuracy: 0.550000\n",
      "Epoch 689, CIFAR-10 Batch 1:  Loss:     1.1856 Validation Accuracy: 0.575000\n",
      "Epoch 690, CIFAR-10 Batch 1:  Loss:     1.2775 Validation Accuracy: 0.575000\n",
      "Epoch 691, CIFAR-10 Batch 1:  Loss:     1.2553 Validation Accuracy: 0.525000\n",
      "Epoch 692, CIFAR-10 Batch 1:  Loss:     1.2779 Validation Accuracy: 0.500000\n",
      "Epoch 693, CIFAR-10 Batch 1:  Loss:     1.2745 Validation Accuracy: 0.525000\n",
      "Epoch 694, CIFAR-10 Batch 1:  Loss:     1.2324 Validation Accuracy: 0.525000\n",
      "Epoch 695, CIFAR-10 Batch 1:  Loss:     1.2694 Validation Accuracy: 0.475000\n",
      "Epoch 696, CIFAR-10 Batch 1:  Loss:     1.2899 Validation Accuracy: 0.475000\n",
      "Epoch 697, CIFAR-10 Batch 1:  Loss:     1.2878 Validation Accuracy: 0.525000\n",
      "Epoch 698, CIFAR-10 Batch 1:  Loss:     1.2927 Validation Accuracy: 0.525000\n",
      "Epoch 699, CIFAR-10 Batch 1:  Loss:     1.2926 Validation Accuracy: 0.450000\n",
      "Epoch 700, CIFAR-10 Batch 1:  Loss:     1.2817 Validation Accuracy: 0.500000\n",
      "Epoch 701, CIFAR-10 Batch 1:  Loss:     1.2782 Validation Accuracy: 0.450000\n",
      "Epoch 702, CIFAR-10 Batch 1:  Loss:     1.2832 Validation Accuracy: 0.450000\n",
      "Epoch 703, CIFAR-10 Batch 1:  Loss:     1.2669 Validation Accuracy: 0.475000\n",
      "Epoch 704, CIFAR-10 Batch 1:  Loss:     1.2664 Validation Accuracy: 0.475000\n",
      "Epoch 705, CIFAR-10 Batch 1:  Loss:     1.2961 Validation Accuracy: 0.500000\n",
      "Epoch 706, CIFAR-10 Batch 1:  Loss:     1.2129 Validation Accuracy: 0.550000\n",
      "Epoch 707, CIFAR-10 Batch 1:  Loss:     1.2304 Validation Accuracy: 0.525000\n",
      "Epoch 708, CIFAR-10 Batch 1:  Loss:     1.2342 Validation Accuracy: 0.550000\n",
      "Epoch 709, CIFAR-10 Batch 1:  Loss:     1.3011 Validation Accuracy: 0.500000\n",
      "Epoch 710, CIFAR-10 Batch 1:  Loss:     1.2400 Validation Accuracy: 0.475000\n",
      "Epoch 711, CIFAR-10 Batch 1:  Loss:     1.2738 Validation Accuracy: 0.525000\n",
      "Epoch 712, CIFAR-10 Batch 1:  Loss:     1.2766 Validation Accuracy: 0.450000\n",
      "Epoch 713, CIFAR-10 Batch 1:  Loss:     1.2817 Validation Accuracy: 0.475000\n",
      "Epoch 714, CIFAR-10 Batch 1:  Loss:     1.2413 Validation Accuracy: 0.550000\n",
      "Epoch 715, CIFAR-10 Batch 1:  Loss:     1.2518 Validation Accuracy: 0.550000\n",
      "Epoch 716, CIFAR-10 Batch 1:  Loss:     1.1981 Validation Accuracy: 0.525000\n",
      "Epoch 717, CIFAR-10 Batch 1:  Loss:     1.2329 Validation Accuracy: 0.575000\n",
      "Epoch 718, CIFAR-10 Batch 1:  Loss:     1.3174 Validation Accuracy: 0.450000\n",
      "Epoch 719, CIFAR-10 Batch 1:  Loss:     1.2408 Validation Accuracy: 0.450000\n",
      "Epoch 720, CIFAR-10 Batch 1:  Loss:     1.2840 Validation Accuracy: 0.500000\n",
      "Epoch 721, CIFAR-10 Batch 1:  Loss:     1.2667 Validation Accuracy: 0.475000\n",
      "Epoch 722, CIFAR-10 Batch 1:  Loss:     1.3114 Validation Accuracy: 0.500000\n",
      "Epoch 723, CIFAR-10 Batch 1:  Loss:     1.2137 Validation Accuracy: 0.575000\n",
      "Epoch 724, CIFAR-10 Batch 1:  Loss:     1.2891 Validation Accuracy: 0.500000\n",
      "Epoch 725, CIFAR-10 Batch 1:  Loss:     1.2490 Validation Accuracy: 0.575000\n",
      "Epoch 726, CIFAR-10 Batch 1:  Loss:     1.2601 Validation Accuracy: 0.525000\n",
      "Epoch 727, CIFAR-10 Batch 1:  Loss:     1.2919 Validation Accuracy: 0.475000\n",
      "Epoch 728, CIFAR-10 Batch 1:  Loss:     1.2719 Validation Accuracy: 0.550000\n",
      "Epoch 729, CIFAR-10 Batch 1:  Loss:     1.2660 Validation Accuracy: 0.525000\n",
      "Epoch 730, CIFAR-10 Batch 1:  Loss:     1.2731 Validation Accuracy: 0.450000\n",
      "Epoch 731, CIFAR-10 Batch 1:  Loss:     1.2433 Validation Accuracy: 0.575000\n",
      "Epoch 732, CIFAR-10 Batch 1:  Loss:     1.2824 Validation Accuracy: 0.450000\n",
      "Epoch 733, CIFAR-10 Batch 1:  Loss:     1.2448 Validation Accuracy: 0.500000\n",
      "Epoch 734, CIFAR-10 Batch 1:  Loss:     1.2311 Validation Accuracy: 0.525000\n",
      "Epoch 735, CIFAR-10 Batch 1:  Loss:     1.2053 Validation Accuracy: 0.525000\n",
      "Epoch 736, CIFAR-10 Batch 1:  Loss:     1.1818 Validation Accuracy: 0.525000\n",
      "Epoch 737, CIFAR-10 Batch 1:  Loss:     1.2531 Validation Accuracy: 0.475000\n",
      "Epoch 738, CIFAR-10 Batch 1:  Loss:     1.2107 Validation Accuracy: 0.500000\n",
      "Epoch 739, CIFAR-10 Batch 1:  Loss:     1.2572 Validation Accuracy: 0.475000\n",
      "Epoch 740, CIFAR-10 Batch 1:  Loss:     1.2165 Validation Accuracy: 0.525000\n",
      "Epoch 741, CIFAR-10 Batch 1:  Loss:     1.2027 Validation Accuracy: 0.575000\n",
      "Epoch 742, CIFAR-10 Batch 1:  Loss:     1.1860 Validation Accuracy: 0.575000\n",
      "Epoch 743, CIFAR-10 Batch 1:  Loss:     1.1941 Validation Accuracy: 0.550000\n",
      "Epoch 744, CIFAR-10 Batch 1:  Loss:     1.2642 Validation Accuracy: 0.575000\n",
      "Epoch 745, CIFAR-10 Batch 1:  Loss:     1.2417 Validation Accuracy: 0.550000\n",
      "Epoch 746, CIFAR-10 Batch 1:  Loss:     1.2616 Validation Accuracy: 0.550000\n",
      "Epoch 747, CIFAR-10 Batch 1:  Loss:     1.2308 Validation Accuracy: 0.550000\n",
      "Epoch 748, CIFAR-10 Batch 1:  Loss:     1.2585 Validation Accuracy: 0.525000\n",
      "Epoch 749, CIFAR-10 Batch 1:  Loss:     1.3491 Validation Accuracy: 0.450000\n",
      "Epoch 750, CIFAR-10 Batch 1:  Loss:     1.2920 Validation Accuracy: 0.500000\n",
      "Epoch 751, CIFAR-10 Batch 1:  Loss:     1.2608 Validation Accuracy: 0.525000\n",
      "Epoch 752, CIFAR-10 Batch 1:  Loss:     1.2726 Validation Accuracy: 0.475000\n",
      "Epoch 753, CIFAR-10 Batch 1:  Loss:     1.1878 Validation Accuracy: 0.500000\n",
      "Epoch 754, CIFAR-10 Batch 1:  Loss:     1.2162 Validation Accuracy: 0.475000\n",
      "Epoch 755, CIFAR-10 Batch 1:  Loss:     1.2492 Validation Accuracy: 0.475000\n",
      "Epoch 756, CIFAR-10 Batch 1:  Loss:     1.2513 Validation Accuracy: 0.500000\n",
      "Epoch 757, CIFAR-10 Batch 1:  Loss:     1.2835 Validation Accuracy: 0.500000\n",
      "Epoch 758, CIFAR-10 Batch 1:  Loss:     1.1993 Validation Accuracy: 0.525000\n",
      "Epoch 759, CIFAR-10 Batch 1:  Loss:     1.1661 Validation Accuracy: 0.575000\n",
      "Epoch 760, CIFAR-10 Batch 1:  Loss:     1.2465 Validation Accuracy: 0.550000\n",
      "Epoch 761, CIFAR-10 Batch 1:  Loss:     1.2255 Validation Accuracy: 0.525000\n",
      "Epoch 762, CIFAR-10 Batch 1:  Loss:     1.1936 Validation Accuracy: 0.475000\n",
      "Epoch 763, CIFAR-10 Batch 1:  Loss:     1.2069 Validation Accuracy: 0.525000\n",
      "Epoch 764, CIFAR-10 Batch 1:  Loss:     1.2880 Validation Accuracy: 0.525000\n",
      "Epoch 765, CIFAR-10 Batch 1:  Loss:     1.2307 Validation Accuracy: 0.500000\n",
      "Epoch 766, CIFAR-10 Batch 1:  Loss:     1.2052 Validation Accuracy: 0.525000\n",
      "Epoch 767, CIFAR-10 Batch 1:  Loss:     1.2471 Validation Accuracy: 0.500000\n",
      "Epoch 768, CIFAR-10 Batch 1:  Loss:     1.2738 Validation Accuracy: 0.500000\n",
      "Epoch 769, CIFAR-10 Batch 1:  Loss:     1.2188 Validation Accuracy: 0.500000\n",
      "Epoch 770, CIFAR-10 Batch 1:  Loss:     1.2179 Validation Accuracy: 0.475000\n",
      "Epoch 771, CIFAR-10 Batch 1:  Loss:     1.2155 Validation Accuracy: 0.475000\n",
      "Epoch 772, CIFAR-10 Batch 1:  Loss:     1.2430 Validation Accuracy: 0.525000\n",
      "Epoch 773, CIFAR-10 Batch 1:  Loss:     1.2223 Validation Accuracy: 0.525000\n",
      "Epoch 774, CIFAR-10 Batch 1:  Loss:     1.2758 Validation Accuracy: 0.475000\n",
      "Epoch 775, CIFAR-10 Batch 1:  Loss:     1.1714 Validation Accuracy: 0.575000\n",
      "Epoch 776, CIFAR-10 Batch 1:  Loss:     1.1935 Validation Accuracy: 0.550000\n",
      "Epoch 777, CIFAR-10 Batch 1:  Loss:     1.2450 Validation Accuracy: 0.500000\n",
      "Epoch 778, CIFAR-10 Batch 1:  Loss:     1.2344 Validation Accuracy: 0.500000\n",
      "Epoch 779, CIFAR-10 Batch 1:  Loss:     1.2212 Validation Accuracy: 0.500000\n",
      "Epoch 780, CIFAR-10 Batch 1:  Loss:     1.3112 Validation Accuracy: 0.475000\n",
      "Epoch 781, CIFAR-10 Batch 1:  Loss:     1.2645 Validation Accuracy: 0.450000\n",
      "Epoch 782, CIFAR-10 Batch 1:  Loss:     1.2523 Validation Accuracy: 0.450000\n",
      "Epoch 783, CIFAR-10 Batch 1:  Loss:     1.2210 Validation Accuracy: 0.475000\n",
      "Epoch 784, CIFAR-10 Batch 1:  Loss:     1.2266 Validation Accuracy: 0.500000\n",
      "Epoch 785, CIFAR-10 Batch 1:  Loss:     1.2379 Validation Accuracy: 0.475000\n",
      "Epoch 786, CIFAR-10 Batch 1:  Loss:     1.2815 Validation Accuracy: 0.550000\n",
      "Epoch 787, CIFAR-10 Batch 1:  Loss:     1.2453 Validation Accuracy: 0.525000\n",
      "Epoch 788, CIFAR-10 Batch 1:  Loss:     1.3356 Validation Accuracy: 0.475000\n",
      "Epoch 789, CIFAR-10 Batch 1:  Loss:     1.2737 Validation Accuracy: 0.500000\n",
      "Epoch 790, CIFAR-10 Batch 1:  Loss:     1.1842 Validation Accuracy: 0.550000\n",
      "Epoch 791, CIFAR-10 Batch 1:  Loss:     1.2636 Validation Accuracy: 0.525000\n",
      "Epoch 792, CIFAR-10 Batch 1:  Loss:     1.2490 Validation Accuracy: 0.575000\n",
      "Epoch 793, CIFAR-10 Batch 1:  Loss:     1.2516 Validation Accuracy: 0.500000\n",
      "Epoch 794, CIFAR-10 Batch 1:  Loss:     1.2408 Validation Accuracy: 0.550000\n",
      "Epoch 795, CIFAR-10 Batch 1:  Loss:     1.2158 Validation Accuracy: 0.525000\n",
      "Epoch 796, CIFAR-10 Batch 1:  Loss:     1.2940 Validation Accuracy: 0.500000\n",
      "Epoch 797, CIFAR-10 Batch 1:  Loss:     1.2457 Validation Accuracy: 0.475000\n",
      "Epoch 798, CIFAR-10 Batch 1:  Loss:     1.1959 Validation Accuracy: 0.575000\n",
      "Epoch 799, CIFAR-10 Batch 1:  Loss:     1.2246 Validation Accuracy: 0.525000\n",
      "Epoch 800, CIFAR-10 Batch 1:  Loss:     1.2019 Validation Accuracy: 0.550000\n",
      "Epoch 801, CIFAR-10 Batch 1:  Loss:     1.2201 Validation Accuracy: 0.500000\n",
      "Epoch 802, CIFAR-10 Batch 1:  Loss:     1.2984 Validation Accuracy: 0.500000\n",
      "Epoch 803, CIFAR-10 Batch 1:  Loss:     1.2236 Validation Accuracy: 0.575000\n",
      "Epoch 804, CIFAR-10 Batch 1:  Loss:     1.2635 Validation Accuracy: 0.500000\n",
      "Epoch 805, CIFAR-10 Batch 1:  Loss:     1.2922 Validation Accuracy: 0.450000\n",
      "Epoch 806, CIFAR-10 Batch 1:  Loss:     1.2236 Validation Accuracy: 0.475000\n",
      "Epoch 807, CIFAR-10 Batch 1:  Loss:     1.2065 Validation Accuracy: 0.475000\n",
      "Epoch 808, CIFAR-10 Batch 1:  Loss:     1.2358 Validation Accuracy: 0.525000\n",
      "Epoch 809, CIFAR-10 Batch 1:  Loss:     1.2516 Validation Accuracy: 0.500000\n",
      "Epoch 810, CIFAR-10 Batch 1:  Loss:     1.2258 Validation Accuracy: 0.450000\n",
      "Epoch 811, CIFAR-10 Batch 1:  Loss:     1.2132 Validation Accuracy: 0.475000\n",
      "Epoch 812, CIFAR-10 Batch 1:  Loss:     1.2730 Validation Accuracy: 0.500000\n",
      "Epoch 813, CIFAR-10 Batch 1:  Loss:     1.2353 Validation Accuracy: 0.475000\n",
      "Epoch 814, CIFAR-10 Batch 1:  Loss:     1.2744 Validation Accuracy: 0.500000\n",
      "Epoch 815, CIFAR-10 Batch 1:  Loss:     1.3000 Validation Accuracy: 0.500000\n",
      "Epoch 816, CIFAR-10 Batch 1:  Loss:     1.2963 Validation Accuracy: 0.475000\n",
      "Epoch 817, CIFAR-10 Batch 1:  Loss:     1.2877 Validation Accuracy: 0.475000\n",
      "Epoch 818, CIFAR-10 Batch 1:  Loss:     1.2779 Validation Accuracy: 0.475000\n",
      "Epoch 819, CIFAR-10 Batch 1:  Loss:     1.2420 Validation Accuracy: 0.525000\n",
      "Epoch 820, CIFAR-10 Batch 1:  Loss:     1.2960 Validation Accuracy: 0.475000\n",
      "Epoch 821, CIFAR-10 Batch 1:  Loss:     1.2966 Validation Accuracy: 0.425000\n",
      "Epoch 822, CIFAR-10 Batch 1:  Loss:     1.1971 Validation Accuracy: 0.575000\n",
      "Epoch 823, CIFAR-10 Batch 1:  Loss:     1.2004 Validation Accuracy: 0.475000\n",
      "Epoch 824, CIFAR-10 Batch 1:  Loss:     1.2324 Validation Accuracy: 0.475000\n",
      "Epoch 825, CIFAR-10 Batch 1:  Loss:     1.2300 Validation Accuracy: 0.500000\n",
      "Epoch 826, CIFAR-10 Batch 1:  Loss:     1.2966 Validation Accuracy: 0.475000\n",
      "Epoch 827, CIFAR-10 Batch 1:  Loss:     1.3387 Validation Accuracy: 0.475000\n",
      "Epoch 828, CIFAR-10 Batch 1:  Loss:     1.2906 Validation Accuracy: 0.500000\n",
      "Epoch 829, CIFAR-10 Batch 1:  Loss:     1.2948 Validation Accuracy: 0.500000\n",
      "Epoch 830, CIFAR-10 Batch 1:  Loss:     1.2865 Validation Accuracy: 0.425000\n",
      "Epoch 831, CIFAR-10 Batch 1:  Loss:     1.2481 Validation Accuracy: 0.475000\n",
      "Epoch 832, CIFAR-10 Batch 1:  Loss:     1.1771 Validation Accuracy: 0.600000\n",
      "Epoch 833, CIFAR-10 Batch 1:  Loss:     1.2400 Validation Accuracy: 0.500000\n",
      "Epoch 834, CIFAR-10 Batch 1:  Loss:     1.3241 Validation Accuracy: 0.500000\n",
      "Epoch 835, CIFAR-10 Batch 1:  Loss:     1.2694 Validation Accuracy: 0.475000\n",
      "Epoch 836, CIFAR-10 Batch 1:  Loss:     1.3268 Validation Accuracy: 0.475000\n",
      "Epoch 837, CIFAR-10 Batch 1:  Loss:     1.2299 Validation Accuracy: 0.525000\n",
      "Epoch 838, CIFAR-10 Batch 1:  Loss:     1.2440 Validation Accuracy: 0.475000\n",
      "Epoch 839, CIFAR-10 Batch 1:  Loss:     1.2839 Validation Accuracy: 0.500000\n",
      "Epoch 840, CIFAR-10 Batch 1:  Loss:     1.2857 Validation Accuracy: 0.475000\n",
      "Epoch 841, CIFAR-10 Batch 1:  Loss:     1.2808 Validation Accuracy: 0.475000\n",
      "Epoch 842, CIFAR-10 Batch 1:  Loss:     1.2683 Validation Accuracy: 0.525000\n",
      "Epoch 843, CIFAR-10 Batch 1:  Loss:     1.2055 Validation Accuracy: 0.475000\n",
      "Epoch 844, CIFAR-10 Batch 1:  Loss:     1.2414 Validation Accuracy: 0.525000\n",
      "Epoch 845, CIFAR-10 Batch 1:  Loss:     1.2284 Validation Accuracy: 0.475000\n",
      "Epoch 846, CIFAR-10 Batch 1:  Loss:     1.3066 Validation Accuracy: 0.475000\n",
      "Epoch 847, CIFAR-10 Batch 1:  Loss:     1.2947 Validation Accuracy: 0.500000\n",
      "Epoch 848, CIFAR-10 Batch 1:  Loss:     1.2379 Validation Accuracy: 0.475000\n",
      "Epoch 849, CIFAR-10 Batch 1:  Loss:     1.2518 Validation Accuracy: 0.475000\n",
      "Epoch 850, CIFAR-10 Batch 1:  Loss:     1.2283 Validation Accuracy: 0.475000\n",
      "Epoch 851, CIFAR-10 Batch 1:  Loss:     1.2910 Validation Accuracy: 0.500000\n",
      "Epoch 852, CIFAR-10 Batch 1:  Loss:     1.2916 Validation Accuracy: 0.450000\n",
      "Epoch 853, CIFAR-10 Batch 1:  Loss:     1.2407 Validation Accuracy: 0.525000\n",
      "Epoch 854, CIFAR-10 Batch 1:  Loss:     1.2133 Validation Accuracy: 0.550000\n",
      "Epoch 855, CIFAR-10 Batch 1:  Loss:     1.1873 Validation Accuracy: 0.575000\n",
      "Epoch 856, CIFAR-10 Batch 1:  Loss:     1.2548 Validation Accuracy: 0.450000\n",
      "Epoch 857, CIFAR-10 Batch 1:  Loss:     1.2696 Validation Accuracy: 0.475000\n",
      "Epoch 858, CIFAR-10 Batch 1:  Loss:     1.2680 Validation Accuracy: 0.475000\n",
      "Epoch 859, CIFAR-10 Batch 1:  Loss:     1.2408 Validation Accuracy: 0.475000\n",
      "Epoch 860, CIFAR-10 Batch 1:  Loss:     1.2306 Validation Accuracy: 0.525000\n",
      "Epoch 861, CIFAR-10 Batch 1:  Loss:     1.2693 Validation Accuracy: 0.450000\n",
      "Epoch 862, CIFAR-10 Batch 1:  Loss:     1.2306 Validation Accuracy: 0.525000\n",
      "Epoch 863, CIFAR-10 Batch 1:  Loss:     1.2634 Validation Accuracy: 0.475000\n",
      "Epoch 864, CIFAR-10 Batch 1:  Loss:     1.2235 Validation Accuracy: 0.475000\n",
      "Epoch 865, CIFAR-10 Batch 1:  Loss:     1.2780 Validation Accuracy: 0.450000\n",
      "Epoch 866, CIFAR-10 Batch 1:  Loss:     1.2836 Validation Accuracy: 0.500000\n",
      "Epoch 867, CIFAR-10 Batch 1:  Loss:     1.1962 Validation Accuracy: 0.525000\n",
      "Epoch 868, CIFAR-10 Batch 1:  Loss:     1.2317 Validation Accuracy: 0.475000\n",
      "Epoch 869, CIFAR-10 Batch 1:  Loss:     1.2169 Validation Accuracy: 0.500000\n",
      "Epoch 870, CIFAR-10 Batch 1:  Loss:     1.2336 Validation Accuracy: 0.475000\n",
      "Epoch 871, CIFAR-10 Batch 1:  Loss:     1.2834 Validation Accuracy: 0.450000\n",
      "Epoch 872, CIFAR-10 Batch 1:  Loss:     1.2352 Validation Accuracy: 0.500000\n",
      "Epoch 873, CIFAR-10 Batch 1:  Loss:     1.2536 Validation Accuracy: 0.450000\n",
      "Epoch 874, CIFAR-10 Batch 1:  Loss:     1.2599 Validation Accuracy: 0.475000\n",
      "Epoch 875, CIFAR-10 Batch 1:  Loss:     1.2280 Validation Accuracy: 0.500000\n",
      "Epoch 876, CIFAR-10 Batch 1:  Loss:     1.2395 Validation Accuracy: 0.500000\n",
      "Epoch 877, CIFAR-10 Batch 1:  Loss:     1.2872 Validation Accuracy: 0.450000\n",
      "Epoch 878, CIFAR-10 Batch 1:  Loss:     1.2660 Validation Accuracy: 0.475000\n",
      "Epoch 879, CIFAR-10 Batch 1:  Loss:     1.3191 Validation Accuracy: 0.450000\n",
      "Epoch 880, CIFAR-10 Batch 1:  Loss:     1.2920 Validation Accuracy: 0.475000\n",
      "Epoch 881, CIFAR-10 Batch 1:  Loss:     1.2975 Validation Accuracy: 0.500000\n",
      "Epoch 882, CIFAR-10 Batch 1:  Loss:     1.2309 Validation Accuracy: 0.525000\n",
      "Epoch 883, CIFAR-10 Batch 1:  Loss:     1.2432 Validation Accuracy: 0.475000\n",
      "Epoch 884, CIFAR-10 Batch 1:  Loss:     1.2166 Validation Accuracy: 0.500000\n",
      "Epoch 885, CIFAR-10 Batch 1:  Loss:     1.2405 Validation Accuracy: 0.500000\n",
      "Epoch 886, CIFAR-10 Batch 1:  Loss:     1.2416 Validation Accuracy: 0.475000\n",
      "Epoch 887, CIFAR-10 Batch 1:  Loss:     1.2806 Validation Accuracy: 0.525000\n",
      "Epoch 888, CIFAR-10 Batch 1:  Loss:     1.3256 Validation Accuracy: 0.500000\n",
      "Epoch 889, CIFAR-10 Batch 1:  Loss:     1.2158 Validation Accuracy: 0.525000\n",
      "Epoch 890, CIFAR-10 Batch 1:  Loss:     1.2063 Validation Accuracy: 0.525000\n",
      "Epoch 891, CIFAR-10 Batch 1:  Loss:     1.2412 Validation Accuracy: 0.425000\n",
      "Epoch 892, CIFAR-10 Batch 1:  Loss:     1.2889 Validation Accuracy: 0.475000\n",
      "Epoch 893, CIFAR-10 Batch 1:  Loss:     1.3035 Validation Accuracy: 0.475000\n",
      "Epoch 894, CIFAR-10 Batch 1:  Loss:     1.2604 Validation Accuracy: 0.500000\n",
      "Epoch 895, CIFAR-10 Batch 1:  Loss:     1.2434 Validation Accuracy: 0.450000\n",
      "Epoch 896, CIFAR-10 Batch 1:  Loss:     1.2170 Validation Accuracy: 0.425000\n",
      "Epoch 897, CIFAR-10 Batch 1:  Loss:     1.1748 Validation Accuracy: 0.500000\n",
      "Epoch 898, CIFAR-10 Batch 1:  Loss:     1.2294 Validation Accuracy: 0.500000\n",
      "Epoch 899, CIFAR-10 Batch 1:  Loss:     1.2545 Validation Accuracy: 0.475000\n",
      "Epoch 900, CIFAR-10 Batch 1:  Loss:     1.2772 Validation Accuracy: 0.475000\n",
      "Epoch 901, CIFAR-10 Batch 1:  Loss:     1.2279 Validation Accuracy: 0.525000\n",
      "Epoch 902, CIFAR-10 Batch 1:  Loss:     1.2455 Validation Accuracy: 0.500000\n",
      "Epoch 903, CIFAR-10 Batch 1:  Loss:     1.2341 Validation Accuracy: 0.525000\n",
      "Epoch 904, CIFAR-10 Batch 1:  Loss:     1.2720 Validation Accuracy: 0.475000\n",
      "Epoch 905, CIFAR-10 Batch 1:  Loss:     1.2284 Validation Accuracy: 0.500000\n",
      "Epoch 906, CIFAR-10 Batch 1:  Loss:     1.2609 Validation Accuracy: 0.500000\n",
      "Epoch 907, CIFAR-10 Batch 1:  Loss:     1.2133 Validation Accuracy: 0.600000\n",
      "Epoch 908, CIFAR-10 Batch 1:  Loss:     1.2031 Validation Accuracy: 0.500000\n",
      "Epoch 909, CIFAR-10 Batch 1:  Loss:     1.1675 Validation Accuracy: 0.550000\n",
      "Epoch 910, CIFAR-10 Batch 1:  Loss:     1.1755 Validation Accuracy: 0.550000\n",
      "Epoch 911, CIFAR-10 Batch 1:  Loss:     1.2387 Validation Accuracy: 0.500000\n",
      "Epoch 912, CIFAR-10 Batch 1:  Loss:     1.2753 Validation Accuracy: 0.475000\n",
      "Epoch 913, CIFAR-10 Batch 1:  Loss:     1.2591 Validation Accuracy: 0.475000\n",
      "Epoch 914, CIFAR-10 Batch 1:  Loss:     1.1600 Validation Accuracy: 0.650000\n",
      "Epoch 915, CIFAR-10 Batch 1:  Loss:     1.2260 Validation Accuracy: 0.450000\n",
      "Epoch 916, CIFAR-10 Batch 1:  Loss:     1.1762 Validation Accuracy: 0.525000\n",
      "Epoch 917, CIFAR-10 Batch 1:  Loss:     1.3057 Validation Accuracy: 0.525000\n",
      "Epoch 918, CIFAR-10 Batch 1:  Loss:     1.1402 Validation Accuracy: 0.550000\n",
      "Epoch 919, CIFAR-10 Batch 1:  Loss:     1.2495 Validation Accuracy: 0.475000\n",
      "Epoch 920, CIFAR-10 Batch 1:  Loss:     1.2451 Validation Accuracy: 0.500000\n",
      "Epoch 921, CIFAR-10 Batch 1:  Loss:     1.2274 Validation Accuracy: 0.500000\n",
      "Epoch 922, CIFAR-10 Batch 1:  Loss:     1.3728 Validation Accuracy: 0.450000\n",
      "Epoch 923, CIFAR-10 Batch 1:  Loss:     1.2023 Validation Accuracy: 0.500000\n",
      "Epoch 924, CIFAR-10 Batch 1:  Loss:     1.2188 Validation Accuracy: 0.500000\n",
      "Epoch 925, CIFAR-10 Batch 1:  Loss:     1.3835 Validation Accuracy: 0.425000\n",
      "Epoch 926, CIFAR-10 Batch 1:  Loss:     1.2441 Validation Accuracy: 0.475000\n",
      "Epoch 927, CIFAR-10 Batch 1:  Loss:     1.2878 Validation Accuracy: 0.500000\n",
      "Epoch 928, CIFAR-10 Batch 1:  Loss:     1.2622 Validation Accuracy: 0.425000\n",
      "Epoch 929, CIFAR-10 Batch 1:  Loss:     1.2551 Validation Accuracy: 0.425000\n",
      "Epoch 930, CIFAR-10 Batch 1:  Loss:     1.1894 Validation Accuracy: 0.525000\n",
      "Epoch 931, CIFAR-10 Batch 1:  Loss:     1.2484 Validation Accuracy: 0.525000\n",
      "Epoch 932, CIFAR-10 Batch 1:  Loss:     1.2021 Validation Accuracy: 0.525000\n",
      "Epoch 933, CIFAR-10 Batch 1:  Loss:     1.2909 Validation Accuracy: 0.450000\n",
      "Epoch 934, CIFAR-10 Batch 1:  Loss:     1.2184 Validation Accuracy: 0.475000\n",
      "Epoch 935, CIFAR-10 Batch 1:  Loss:     1.1897 Validation Accuracy: 0.500000\n",
      "Epoch 936, CIFAR-10 Batch 1:  Loss:     1.2299 Validation Accuracy: 0.475000\n",
      "Epoch 937, CIFAR-10 Batch 1:  Loss:     1.2425 Validation Accuracy: 0.500000\n",
      "Epoch 938, CIFAR-10 Batch 1:  Loss:     1.2076 Validation Accuracy: 0.575000\n",
      "Epoch 939, CIFAR-10 Batch 1:  Loss:     1.2759 Validation Accuracy: 0.475000\n",
      "Epoch 940, CIFAR-10 Batch 1:  Loss:     1.2413 Validation Accuracy: 0.500000\n",
      "Epoch 941, CIFAR-10 Batch 1:  Loss:     1.2475 Validation Accuracy: 0.450000\n",
      "Epoch 942, CIFAR-10 Batch 1:  Loss:     1.1590 Validation Accuracy: 0.550000\n",
      "Epoch 943, CIFAR-10 Batch 1:  Loss:     1.2174 Validation Accuracy: 0.475000\n",
      "Epoch 944, CIFAR-10 Batch 1:  Loss:     1.2635 Validation Accuracy: 0.475000\n",
      "Epoch 945, CIFAR-10 Batch 1:  Loss:     1.2776 Validation Accuracy: 0.475000\n",
      "Epoch 946, CIFAR-10 Batch 1:  Loss:     1.2827 Validation Accuracy: 0.500000\n",
      "Epoch 947, CIFAR-10 Batch 1:  Loss:     1.2707 Validation Accuracy: 0.525000\n",
      "Epoch 948, CIFAR-10 Batch 1:  Loss:     1.2099 Validation Accuracy: 0.575000\n",
      "Epoch 949, CIFAR-10 Batch 1:  Loss:     1.2711 Validation Accuracy: 0.525000\n",
      "Epoch 950, CIFAR-10 Batch 1:  Loss:     1.2609 Validation Accuracy: 0.500000\n",
      "Epoch 951, CIFAR-10 Batch 1:  Loss:     1.2343 Validation Accuracy: 0.500000\n",
      "Epoch 952, CIFAR-10 Batch 1:  Loss:     1.2333 Validation Accuracy: 0.525000\n",
      "Epoch 953, CIFAR-10 Batch 1:  Loss:     1.2096 Validation Accuracy: 0.500000\n",
      "Epoch 954, CIFAR-10 Batch 1:  Loss:     1.2983 Validation Accuracy: 0.500000\n",
      "Epoch 955, CIFAR-10 Batch 1:  Loss:     1.2623 Validation Accuracy: 0.550000\n",
      "Epoch 956, CIFAR-10 Batch 1:  Loss:     1.2736 Validation Accuracy: 0.525000\n",
      "Epoch 957, CIFAR-10 Batch 1:  Loss:     1.2672 Validation Accuracy: 0.525000\n",
      "Epoch 958, CIFAR-10 Batch 1:  Loss:     1.2382 Validation Accuracy: 0.475000\n",
      "Epoch 959, CIFAR-10 Batch 1:  Loss:     1.2504 Validation Accuracy: 0.575000\n",
      "Epoch 960, CIFAR-10 Batch 1:  Loss:     1.2516 Validation Accuracy: 0.525000\n",
      "Epoch 961, CIFAR-10 Batch 1:  Loss:     1.2143 Validation Accuracy: 0.550000\n",
      "Epoch 962, CIFAR-10 Batch 1:  Loss:     1.2055 Validation Accuracy: 0.550000\n",
      "Epoch 963, CIFAR-10 Batch 1:  Loss:     1.2894 Validation Accuracy: 0.450000\n",
      "Epoch 964, CIFAR-10 Batch 1:  Loss:     1.2400 Validation Accuracy: 0.500000\n",
      "Epoch 965, CIFAR-10 Batch 1:  Loss:     1.2103 Validation Accuracy: 0.550000\n",
      "Epoch 966, CIFAR-10 Batch 1:  Loss:     1.2886 Validation Accuracy: 0.550000\n",
      "Epoch 967, CIFAR-10 Batch 1:  Loss:     1.1921 Validation Accuracy: 0.575000\n",
      "Epoch 968, CIFAR-10 Batch 1:  Loss:     1.2143 Validation Accuracy: 0.550000\n",
      "Epoch 969, CIFAR-10 Batch 1:  Loss:     1.2100 Validation Accuracy: 0.525000\n",
      "Epoch 970, CIFAR-10 Batch 1:  Loss:     1.2981 Validation Accuracy: 0.450000\n",
      "Epoch 971, CIFAR-10 Batch 1:  Loss:     1.2222 Validation Accuracy: 0.500000\n",
      "Epoch 972, CIFAR-10 Batch 1:  Loss:     1.2632 Validation Accuracy: 0.475000\n",
      "Epoch 973, CIFAR-10 Batch 1:  Loss:     1.2166 Validation Accuracy: 0.500000\n",
      "Epoch 974, CIFAR-10 Batch 1:  Loss:     1.2221 Validation Accuracy: 0.525000\n",
      "Epoch 975, CIFAR-10 Batch 1:  Loss:     1.2529 Validation Accuracy: 0.550000\n",
      "Epoch 976, CIFAR-10 Batch 1:  Loss:     1.2822 Validation Accuracy: 0.500000\n",
      "Epoch 977, CIFAR-10 Batch 1:  Loss:     1.2675 Validation Accuracy: 0.550000\n",
      "Epoch 978, CIFAR-10 Batch 1:  Loss:     1.2444 Validation Accuracy: 0.475000\n",
      "Epoch 979, CIFAR-10 Batch 1:  Loss:     1.2051 Validation Accuracy: 0.525000\n",
      "Epoch 980, CIFAR-10 Batch 1:  Loss:     1.3017 Validation Accuracy: 0.450000\n",
      "Epoch 981, CIFAR-10 Batch 1:  Loss:     1.2454 Validation Accuracy: 0.525000\n",
      "Epoch 982, CIFAR-10 Batch 1:  Loss:     1.2576 Validation Accuracy: 0.525000\n",
      "Epoch 983, CIFAR-10 Batch 1:  Loss:     1.2335 Validation Accuracy: 0.575000\n",
      "Epoch 984, CIFAR-10 Batch 1:  Loss:     1.2626 Validation Accuracy: 0.525000\n",
      "Epoch 985, CIFAR-10 Batch 1:  Loss:     1.2900 Validation Accuracy: 0.500000\n",
      "Epoch 986, CIFAR-10 Batch 1:  Loss:     1.2559 Validation Accuracy: 0.525000\n",
      "Epoch 987, CIFAR-10 Batch 1:  Loss:     1.3240 Validation Accuracy: 0.550000\n",
      "Epoch 988, CIFAR-10 Batch 1:  Loss:     1.2687 Validation Accuracy: 0.525000\n",
      "Epoch 989, CIFAR-10 Batch 1:  Loss:     1.2479 Validation Accuracy: 0.525000\n",
      "Epoch 990, CIFAR-10 Batch 1:  Loss:     1.2482 Validation Accuracy: 0.500000\n",
      "Epoch 991, CIFAR-10 Batch 1:  Loss:     1.3435 Validation Accuracy: 0.475000\n",
      "Epoch 992, CIFAR-10 Batch 1:  Loss:     1.2214 Validation Accuracy: 0.550000\n",
      "Epoch 993, CIFAR-10 Batch 1:  Loss:     1.2315 Validation Accuracy: 0.525000\n",
      "Epoch 994, CIFAR-10 Batch 1:  Loss:     1.3278 Validation Accuracy: 0.500000\n",
      "Epoch 995, CIFAR-10 Batch 1:  Loss:     1.2685 Validation Accuracy: 0.475000\n",
      "Epoch 996, CIFAR-10 Batch 1:  Loss:     1.2515 Validation Accuracy: 0.500000\n",
      "Epoch 997, CIFAR-10 Batch 1:  Loss:     1.2413 Validation Accuracy: 0.500000\n",
      "Epoch 998, CIFAR-10 Batch 1:  Loss:     1.2290 Validation Accuracy: 0.525000\n",
      "Epoch 999, CIFAR-10 Batch 1:  Loss:     1.2676 Validation Accuracy: 0.450000\n",
      "Epoch 1000, CIFAR-10 Batch 1:  Loss:     1.2857 Validation Accuracy: 0.500000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2736 Validation Accuracy: 0.150000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.0346 Validation Accuracy: 0.275000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.8577 Validation Accuracy: 0.350000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.8920 Validation Accuracy: 0.225000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.8515 Validation Accuracy: 0.350000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0653 Validation Accuracy: 0.225000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9237 Validation Accuracy: 0.225000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.8304 Validation Accuracy: 0.300000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.8283 Validation Accuracy: 0.350000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.8195 Validation Accuracy: 0.300000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9880 Validation Accuracy: 0.275000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.8365 Validation Accuracy: 0.325000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.7370 Validation Accuracy: 0.200000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.7947 Validation Accuracy: 0.325000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7530 Validation Accuracy: 0.350000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9456 Validation Accuracy: 0.250000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.7856 Validation Accuracy: 0.300000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.7011 Validation Accuracy: 0.350000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.7826 Validation Accuracy: 0.275000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.7153 Validation Accuracy: 0.300000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8764 Validation Accuracy: 0.250000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.7711 Validation Accuracy: 0.350000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.6634 Validation Accuracy: 0.350000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.7209 Validation Accuracy: 0.325000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6903 Validation Accuracy: 0.375000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8677 Validation Accuracy: 0.300000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.7426 Validation Accuracy: 0.350000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.6693 Validation Accuracy: 0.325000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.7186 Validation Accuracy: 0.350000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.7080 Validation Accuracy: 0.425000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.8379 Validation Accuracy: 0.350000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.7306 Validation Accuracy: 0.400000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.6142 Validation Accuracy: 0.350000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.6959 Validation Accuracy: 0.375000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.6694 Validation Accuracy: 0.500000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8160 Validation Accuracy: 0.300000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.6995 Validation Accuracy: 0.375000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.6201 Validation Accuracy: 0.400000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.6679 Validation Accuracy: 0.375000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.6626 Validation Accuracy: 0.475000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.7712 Validation Accuracy: 0.350000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.6586 Validation Accuracy: 0.425000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.5903 Validation Accuracy: 0.375000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.6098 Validation Accuracy: 0.450000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.6243 Validation Accuracy: 0.425000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7413 Validation Accuracy: 0.425000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.6639 Validation Accuracy: 0.425000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.5636 Validation Accuracy: 0.450000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.6038 Validation Accuracy: 0.425000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.6140 Validation Accuracy: 0.450000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7197 Validation Accuracy: 0.400000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.6775 Validation Accuracy: 0.300000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.5574 Validation Accuracy: 0.425000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.5437 Validation Accuracy: 0.425000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.6087 Validation Accuracy: 0.425000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.6740 Validation Accuracy: 0.425000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.6554 Validation Accuracy: 0.400000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.4938 Validation Accuracy: 0.400000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.5350 Validation Accuracy: 0.500000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.5764 Validation Accuracy: 0.525000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.6337 Validation Accuracy: 0.475000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.6218 Validation Accuracy: 0.400000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.5000 Validation Accuracy: 0.475000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.5006 Validation Accuracy: 0.575000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.5834 Validation Accuracy: 0.425000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5912 Validation Accuracy: 0.450000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.6159 Validation Accuracy: 0.400000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.5154 Validation Accuracy: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.4727 Validation Accuracy: 0.625000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.5353 Validation Accuracy: 0.525000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.6097 Validation Accuracy: 0.450000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.6230 Validation Accuracy: 0.400000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.4067 Validation Accuracy: 0.550000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.4528 Validation Accuracy: 0.525000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.5321 Validation Accuracy: 0.525000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.5690 Validation Accuracy: 0.450000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.5750 Validation Accuracy: 0.425000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.4711 Validation Accuracy: 0.550000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.4489 Validation Accuracy: 0.600000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.4690 Validation Accuracy: 0.525000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.5824 Validation Accuracy: 0.475000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.5583 Validation Accuracy: 0.500000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.3822 Validation Accuracy: 0.575000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.4124 Validation Accuracy: 0.525000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.4876 Validation Accuracy: 0.550000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.5047 Validation Accuracy: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.5844 Validation Accuracy: 0.450000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.3763 Validation Accuracy: 0.550000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.4208 Validation Accuracy: 0.550000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.4817 Validation Accuracy: 0.550000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.5197 Validation Accuracy: 0.475000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.5584 Validation Accuracy: 0.475000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.3769 Validation Accuracy: 0.600000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.4234 Validation Accuracy: 0.475000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.5256 Validation Accuracy: 0.525000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.5316 Validation Accuracy: 0.375000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.5822 Validation Accuracy: 0.425000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.3538 Validation Accuracy: 0.600000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.4003 Validation Accuracy: 0.525000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.4595 Validation Accuracy: 0.525000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.5336 Validation Accuracy: 0.425000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.5453 Validation Accuracy: 0.475000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.3377 Validation Accuracy: 0.550000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.3953 Validation Accuracy: 0.575000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.4674 Validation Accuracy: 0.550000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.4753 Validation Accuracy: 0.450000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.5349 Validation Accuracy: 0.450000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.3112 Validation Accuracy: 0.525000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.3730 Validation Accuracy: 0.525000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.4528 Validation Accuracy: 0.550000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.5154 Validation Accuracy: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.5604 Validation Accuracy: 0.400000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.3069 Validation Accuracy: 0.575000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.3497 Validation Accuracy: 0.600000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.4399 Validation Accuracy: 0.525000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.4509 Validation Accuracy: 0.525000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.5779 Validation Accuracy: 0.450000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.3409 Validation Accuracy: 0.550000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.3086 Validation Accuracy: 0.600000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.4361 Validation Accuracy: 0.475000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.4326 Validation Accuracy: 0.475000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.5223 Validation Accuracy: 0.425000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.3085 Validation Accuracy: 0.575000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.3294 Validation Accuracy: 0.600000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.4850 Validation Accuracy: 0.475000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.4275 Validation Accuracy: 0.500000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.5189 Validation Accuracy: 0.450000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.2849 Validation Accuracy: 0.675000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.3148 Validation Accuracy: 0.625000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.4157 Validation Accuracy: 0.525000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.4437 Validation Accuracy: 0.475000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.4948 Validation Accuracy: 0.475000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.2714 Validation Accuracy: 0.625000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.3056 Validation Accuracy: 0.625000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.4542 Validation Accuracy: 0.550000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.4162 Validation Accuracy: 0.450000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.5067 Validation Accuracy: 0.450000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.2608 Validation Accuracy: 0.700000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.2768 Validation Accuracy: 0.600000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.4311 Validation Accuracy: 0.575000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.4167 Validation Accuracy: 0.525000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.4978 Validation Accuracy: 0.475000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.2585 Validation Accuracy: 0.600000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.2805 Validation Accuracy: 0.575000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.4562 Validation Accuracy: 0.525000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.4301 Validation Accuracy: 0.525000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.4899 Validation Accuracy: 0.375000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.2471 Validation Accuracy: 0.625000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.2895 Validation Accuracy: 0.600000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.4200 Validation Accuracy: 0.525000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.45166015625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j9f0SzVnLCMPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9ab00ef28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
